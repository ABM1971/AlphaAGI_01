{
  "title": "Overcoming Text-Dominant Bias via Modality Reliability Routing for Arbitrary Audio-Text Conditions",
  "abstract": "Audio-Text Large Models (AT-LLMs) often exhibit a 'text-dominant bias,' over-relying on textual input and leading to performance degradation when text is adversarial or irrelevant. We address this by reframing multimodal fusion as a dynamic routing task based on modality reliability. We propose a framework that constructs a 'Text Interference' representation utilizing energy and temporal evidence to quantify cross-modality discrepancies. A lightweight gating mechanism is trained to assess this reliability, dynamically routing inputs to a Joint Multimodal Expert for faithful cases or an Audio-only Expert for adversarial ones. This approach ensures robust emotion recognition under arbitrary modality conditions, significantly outperforming static fusion baselines by prioritizing reliable unimodal evidence when cross-modal alignment fails.",
  "problem_framing": "We reframe the challenge of multimodal emotion recognition from a static feature fusion problem to a dynamic management of arbitrary modality conditions. The critical issue is the text-dominant bias, where the model over-relies on text inputs due to imbalanced dependency structures. This reframing shifts the focus from maximizing cross-modal alignment to identifying and mitigating scenarios where text provides misleading evidence, such as adversarial or irrelevant settings, thereby highlighting the need for robust handling of modality discrepancies.",
  "gap_pattern": "Current approaches to Audio-Text LLMs fail to address text-dominant bias because they treat cross-modal interaction as a static, unconditional process. These methods assume modalities are generally consistent and lack the mechanism to detect when text is misleading. By reframing the fusion process as a fixed aggregation rather than a conditional decision, existing models cannot effectively handle arbitrary modality conditions, leading to performance collapse when the dominant modality (text) conflicts with the target emotion signal in audio.",
  "solution": "Our solution transforms the fusion paradigm by introducing a Modality Reliability Routing framework that actively detects and responds to text-dominant bias. We utilize energy and temporal evidence to construct a 'Text Interference' representation, quantifying the reliability of the text modality relative to audio. This representation drives a lightweight gating mechanism that learns the dependency structure between modalities. During inference, the system dynamically routes the input to either a Joint Multimodal Expert (for faithful, consistent cases) or an Audio-only Expert (for adversarial/irrelevant cases). This approach effectively mitigates bias by leveraging strong unimodal evidence when cross-modal trust breaks down.",
  "method_skeleton": "Re-analyze the inference pipeline of Audio-Text LLMs to incorporate a lightweight gating mechanism that optimizes robustness under arbitrary modality conditions; Utilize a 'Text Interference' framework to model the dependency dynamics between audio and text, extending it to include energy and temporal evidence for reliability assessment; Analyze static fusion limitations and introduce a Dynamic Routing strategy to select between a Joint Multimodal Expert and an Audio-only Expert based on the learned modality trustworthiness.",
  "innovation_claims": [
    "Transform multimodal learning from static fusion to Modality Reliability Routing, effectively mitigating text-dominant bias by dynamically selecting experts based on cross-modal dependency structures.",
    "Reframe the handling of misleading text as a quantifiable 'Text Interference' problem by leveraging energy and temporal evidence, enabling precise assessment of modality trustworthiness.",
    "Shift the robustness paradigm for emotion recognition to handle arbitrary modality conditions, ensuring stability under adversarial and irrelevant settings by prioritizing reliable unimodal audio evidence."
  ],
  "experiments_plan": "We evaluate on IEMOCAP and MELD, constructing adversarial and irrelevant splits to simulate arbitrary modality conditions. Metrics include Accuracy and Weighted F1-score. We compare against standard Audio-Text LLMs and static fusion baselines. Ablation studies will isolate the impact of the interference representation, the gating mechanism, and the routing strategy on robustness."
}