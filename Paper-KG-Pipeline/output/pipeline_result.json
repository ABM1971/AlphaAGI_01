{
  "user_idea": "我们研究音频-文本大模型的文本主导偏见：在 faithful/adversarial/irrelevant 三类设定下，提出基于模态可信度的路由/融合框架。通过构造“文本干扰”表示能量与时序证据，训练轻量门控/适配器，在推理时选择 Joint 或 Audio 专家，实现对抗文本误导下的稳健情绪识别",
  "success": true,
  "iterations": 1,
  "selected_patterns": {
    "stability": [
      "pattern_87",
      "pattern_112",
      "pattern_115",
      "pattern_9",
      "pattern_3"
    ],
    "novelty": [
      "pattern_36",
      "pattern_112",
      "pattern_113",
      "pattern_95",
      "pattern_13"
    ],
    "domain_distance": [
      "pattern_13",
      "pattern_113",
      "pattern_112",
      "pattern_8",
      "pattern_87"
    ]
  },
  "final_story": {
    "title": "Modality Credibility Routing: Mitigating Text-Dominant Bias in Audio-Text Large Language Models via Lightweight Gating for Robust Emotion Recognition",
    "abstract": "Audio-text large language models exhibit a critical text-dominant bias, where models over-rely on textual information while neglecting acoustic cues—analogous to hallucination phenomena in vision-language models. This bias severely degrades performance when text is adversarial, irrelevant, or misaligned with audio content. We present Modality Credibility Routing (MCR), a novel framework that dynamically assesses modality reliability and routes representations to appropriate expert pathways. MCR introduces text interference detection through energy-based features measuring representation confidence and temporal evidence capturing audio-text alignment consistency. A lightweight gating mechanism learns to estimate modality credibility scores, enabling inference-time routing between Joint (audio+text) and Audio-only experts without full model retraining. We systematically evaluate text-dominant bias under three experimental settings—faithful, adversarial, and irrelevant text—establishing the first comprehensive benchmark for audio-text model robustness. Experiments on IEMOCAP, MELD, and CMU-MOSEI with constructed adversarial variants demonstrate that MCR achieves robust emotion recognition while preserving performance on faithful inputs, reducing robustness drop by over 40% compared to standard fusion approaches.",
    "problem_framing": "We reframe multimodal emotion recognition from a simple audio-text fusion problem to a modality credibility challenge. Current audio-text large language models implicitly assume text reliability, treating textual and acoustic modalities as equally trustworthy. However, real-world scenarios frequently present misleading, adversarial, or irrelevant text that contradicts acoustic evidence. This text-dominant bias mirrors the hallucination problem in vision-language models, where modality misalignment leads to unreliable outputs. By reconceptualizing the task as one requiring dynamic credibility assessment—determining when to trust text versus audio—we transform robust emotion recognition into a modality routing problem that demands inference-time adaptation based on detected text interference patterns.",
    "gap_pattern": "Existing audio-text fusion methods fail to address text-dominant bias because they assume static modality contributions. Early and late fusion approaches treat modalities as equally reliable, while prompt-based audio-text LLMs inherit text-over-reliance from their language model foundations. Current debiasing techniques focus on training-time rebalancing rather than inference-time adaptation to adversarial inputs. Critically, hallucination mitigation advances in vision-language models—including latent space steering and multimodal alignment—remain unexplored for audio-text domains. No existing framework provides: (1) systematic evaluation under faithful/adversarial/irrelevant settings, (2) dynamic credibility estimation for modality routing, or (3) lightweight intervention mechanisms that preserve faithful-setting performance while defending against text interference.",
    "solution": "Modality Credibility Routing transforms text-dominant bias mitigation from static fusion rebalancing into dynamic inference-time adaptation. Our framework first constructs 'text interference' representations by extracting energy-based features that measure representation magnitude and confidence divergence between modalities, combined with temporal evidence capturing audio-text alignment consistency across time windows. These features feed into lightweight gating networks trained via contrastive learning on constructed faithful/adversarial/irrelevant triplets, learning to distinguish reliable from misleading text without requiring full model retraining. At inference, the gating mechanism produces modality credibility scores that route inputs to either a Joint expert leveraging both modalities or an Audio-only expert grounded purely in acoustic evidence. This routing strategy, inspired by self-introspective decoding in VLM hallucination mitigation, enables the model to adaptively trust or discount textual information. Optional momentum-based consistency mechanisms stabilize audio representations across layers under text interference, ensuring robust emotion predictions regardless of text reliability conditions.",
    "method_skeleton": "Step 1: Construct text interference representations using energy-based features (representation magnitude, confidence divergence) and temporal evidence (audio-text alignment consistency over sliding windows); Step 2: Train lightweight gating networks and adapters via contrastive learning on faithful/adversarial/irrelevant triplets to estimate modality credibility scores; Step 3: Implement inference-time routing mechanism that selects Joint (audio+text) expert when text credibility is high or Audio-only expert when text interference is detected; Step 4: Apply preference optimization to refine gating decisions by distinguishing reliable from misleading text representations; Step 5: Integrate optional momentum-inspired consistency mechanisms across transformer layers to stabilize audio feature representations under varying text interference conditions",
    "innovation_claims": [
      "Transform the understanding of audio-text LLM reliability from assumed modality balance to systematic text-dominant bias analysis by establishing the first comprehensive experimental framework evaluating model behavior under faithful, adversarial, and irrelevant text settings, revealing critical vulnerabilities in current multimodal fusion approaches",
      "Reframe text interference detection from post-hoc error analysis to proactive credibility estimation by introducing a novel energy-based and temporal evidence framework that quantifies modality reliability, bridging hallucination mitigation techniques from vision-language models to the audio-text domain for the first time",
      "Transform robust multimodal emotion recognition from full model retraining to lightweight inference-time adaptation by developing gating and adapter mechanisms that enable dynamic expert routing based on modality credibility, achieving adversarial robustness with minimal computational overhead while preserving faithful-setting performance"
    ],
    "experiments_plan": "Datasets: IEMOCAP, MELD, CMU-MOSEI with constructed adversarial (emotion-contradicting) and irrelevant (random/unrelated) text variants. Metrics: Accuracy, weighted/unweighted F1, robustness drop, modality reliance ratio. Baselines: Early/late fusion, prompt-based audio-text LLMs, debiasing methods, adapted VLM hallucination techniques. Ablations: gating vs. adapter mechanisms, energy vs. temporal features, routing thresholds, expert selection strategies. Robustness analysis across adversarial intensity levels and cross-dataset generalization."
  },
  "review_history": [
    {
      "pass": true,
      "avg_score": 7.113333333333226,
      "reviews": [
        {
          "reviewer": "Reviewer A",
          "role": "Methodology",
          "score": 7.19999999999989,
          "feedback": "Blind comparisons vs 11 anchors. Loss=5.9920, AvgStrength=1.18. CoachPriority: innovation_claims, method_skeleton, experiments_plan."
        },
        {
          "reviewer": "Reviewer B",
          "role": "Novelty",
          "score": 7.8799999999998755,
          "feedback": "Blind comparisons vs 11 anchors. Loss=5.3843, AvgStrength=1.18. CoachPriority: innovation_claims, method_skeleton, experiments_plan."
        },
        {
          "reviewer": "Reviewer C",
          "role": "Storyteller",
          "score": 6.25999999999991,
          "feedback": "Blind comparisons vs 11 anchors. Loss=6.1520, AvgStrength=1.00. CoachPriority: innovation_claims, method_skeleton, experiments_plan."
        }
      ],
      "main_issue": "domain_distance",
      "suggestions": [
        "从domain_distance维度选择跨域Pattern",
        "引入不同视角优化叙事"
      ],
      "audit": {
        "pattern_id": "pattern_87",
        "anchors": [
          {
            "anchor_id": "A1",
            "paper_id": "oqSKdRyYO1g",
            "score10": 5.6125,
            "weight": 0.50216471526805
          },
          {
            "anchor_id": "A2",
            "paper_id": "3leZITnUE9r",
            "score10": 5.932,
            "weight": 0.5364549309066033
          },
          {
            "anchor_id": "A3",
            "paper_id": "qIN5VDdEOr",
            "score10": 6.01,
            "weight": 0.8610221898474837
          },
          {
            "anchor_id": "A4",
            "paper_id": "TljGdvzFq2",
            "score10": 6.112,
            "weight": 0.7928139244371923
          },
          {
            "anchor_id": "A5",
            "paper_id": "kNHVViEPWK",
            "score10": 6.183999999999999,
            "weight": 0.9430312995937128
          },
          {
            "anchor_id": "A6",
            "paper_id": "fjEZ2LPceZ",
            "score10": 6.292,
            "weight": 0.7928139244371923
          },
          {
            "anchor_id": "A7",
            "paper_id": "98p5x51L5af",
            "score10": 6.4719999999999995,
            "weight": 0.9003816428281685
          },
          {
            "anchor_id": "A8",
            "paper_id": "gye2U9uNXx",
            "score10": 6.544,
            "weight": 0.9430312995937128
          },
          {
            "anchor_id": "A9",
            "paper_id": "OIe3kpwl40D",
            "score10": 7.327000000000001,
            "weight": 0.5293233291663382
          },
          {
            "anchor_id": "A10",
            "paper_id": "RFqeoVfLHa",
            "score10": 6.183999999999999,
            "weight": 0.9430312995937128
          },
          {
            "anchor_id": "A11",
            "paper_id": "URPwT55i6O",
            "score10": 6.183999999999999,
            "weight": 0.9430312995937128
          }
        ],
        "anchors_rounds": [
          [
            {
              "anchor_id": "A1",
              "paper_id": "oqSKdRyYO1g",
              "score10": 5.6125,
              "weight": 0.50216471526805
            },
            {
              "anchor_id": "A2",
              "paper_id": "3leZITnUE9r",
              "score10": 5.932,
              "weight": 0.5364549309066033
            },
            {
              "anchor_id": "A3",
              "paper_id": "qIN5VDdEOr",
              "score10": 6.01,
              "weight": 0.8610221898474837
            },
            {
              "anchor_id": "A4",
              "paper_id": "TljGdvzFq2",
              "score10": 6.112,
              "weight": 0.7928139244371923
            },
            {
              "anchor_id": "A5",
              "paper_id": "kNHVViEPWK",
              "score10": 6.183999999999999,
              "weight": 0.9430312995937128
            },
            {
              "anchor_id": "A6",
              "paper_id": "fjEZ2LPceZ",
              "score10": 6.292,
              "weight": 0.7928139244371923
            },
            {
              "anchor_id": "A7",
              "paper_id": "98p5x51L5af",
              "score10": 6.4719999999999995,
              "weight": 0.9003816428281685
            },
            {
              "anchor_id": "A8",
              "paper_id": "gye2U9uNXx",
              "score10": 6.544,
              "weight": 0.9430312995937128
            },
            {
              "anchor_id": "A9",
              "paper_id": "OIe3kpwl40D",
              "score10": 7.327000000000001,
              "weight": 0.5293233291663382
            },
            {
              "anchor_id": "A10",
              "paper_id": "RFqeoVfLHa",
              "score10": 6.183999999999999,
              "weight": 0.9430312995937128
            },
            {
              "anchor_id": "A11",
              "paper_id": "URPwT55i6O",
              "score10": 6.183999999999999,
              "weight": 0.9430312995937128
            }
          ]
        ],
        "role_details": {
          "Methodology": {
            "comparisons": [
              {
                "anchor_id": "A1",
                "judgement": "better",
                "strength": "medium",
                "rationale": "Story specifies multi-step technical approach with energy-based features, gating networks, and contrastive training; Anchor describes audit without technical detail."
              },
              {
                "anchor_id": "A2",
                "judgement": "tie",
                "strength": "weak",
                "rationale": "Both propose novel metrics with empirical analysis; Story has multimodal components while Anchor has correlation analysis. Comparable methodological specificity."
              },
              {
                "anchor_id": "A3",
                "judgement": "tie",
                "strength": "weak",
                "rationale": "Both analyze internal representations with specific techniques. Story uses energy features and gating; Anchor identifies embedding dimensions. Similar rigor."
              },
              {
                "anchor_id": "A4",
                "judgement": "better",
                "strength": "weak",
                "rationale": "Story provides concrete technical pipeline with contrastive training and adapters; Anchor relies on human annotation without detailed algorithmic approach."
              },
              {
                "anchor_id": "A5",
                "judgement": "tie",
                "strength": "weak",
                "rationale": "Both develop systematic frameworks with multi-step analysis. Story uses energy-based features; Anchor parses rationales. Comparable methodological depth."
              },
              {
                "anchor_id": "A6",
                "judgement": "better",
                "strength": "weak",
                "rationale": "Story details specific technical components and training approach; Anchor focuses on benchmark curation without algorithmic methodology."
              },
              {
                "anchor_id": "A7",
                "judgement": "better",
                "strength": "medium",
                "rationale": "Story provides concrete technical pipeline; Anchor describes prompting strategies without detailed systematic methodology."
              },
              {
                "anchor_id": "A8",
                "judgement": "tie",
                "strength": "weak",
                "rationale": "Both introduce novel detection methods. Story uses energy features and gating; Anchor develops thesaurus approach. Similar methodological specificity."
              },
              {
                "anchor_id": "A9",
                "judgement": "tie",
                "strength": "weak",
                "rationale": "Both propose structured evaluation approaches with clear steps. Story focuses on multimodal; Anchor on sentence matching. Comparable technical clarity."
              },
              {
                "anchor_id": "A10",
                "judgement": "tie",
                "strength": "weak",
                "rationale": "Both develop evaluative frameworks with systematic analysis. Story specifies features and training; Anchor analyzes trajectories. Similar rigor."
              },
              {
                "anchor_id": "A11",
                "judgement": "tie",
                "strength": "weak",
                "rationale": "Both present systematic approaches with statistical components. Story uses contrastive training; Anchor uses MAP estimation. Comparable methodological sophistication."
              }
            ],
            "loss": 5.992036976242362,
            "avg_strength": 1.1818181818181819,
            "monotonic_violations": 2,
            "ci_low": 5.899999999999918,
            "ci_high": 8.789999999999857,
            "tau": 1.0
          },
          "Novelty": {
            "comparisons": [
              {
                "anchor_id": "A1",
                "judgement": "better",
                "strength": "medium",
                "rationale": "Story introduces novel modality credibility framing with energy-based features for LLMs; Anchor audits existing ASR bias without new technical methods."
              },
              {
                "anchor_id": "A2",
                "judgement": "tie",
                "strength": "weak",
                "rationale": "Both reframe existing problems with novel metrics/frameworks for bias analysis; Story focuses on multimodal credibility, Anchor on representational harms."
              },
              {
                "anchor_id": "A3",
                "judgement": "tie",
                "strength": "weak",
                "rationale": "Both identify novel internal dimensions affecting LLM behavior; Story addresses modality credibility, Anchor identifies instruction-following dimension."
              },
              {
                "anchor_id": "A4",
                "judgement": "better",
                "strength": "weak",
                "rationale": "Story proposes novel energy-based interference detection framework; Anchor extends evaluation to cross-capabilities without new technical innovation."
              },
              {
                "anchor_id": "A5",
                "judgement": "tie",
                "strength": "weak",
                "rationale": "Both introduce novel evaluation frameworks; Story focuses on modality credibility, Anchor on skill-level parsing from rationales."
              },
              {
                "anchor_id": "A6",
                "judgement": "better",
                "strength": "medium",
                "rationale": "Story proposes novel modality credibility framework with technical methods; Anchor creates benchmark without methodological innovation."
              },
              {
                "anchor_id": "A7",
                "judgement": "better",
                "strength": "weak",
                "rationale": "Story introduces novel energy-based credibility detection; Anchor applies prompting strategies without fundamentally new technical contributions."
              },
              {
                "anchor_id": "A8",
                "judgement": "tie",
                "strength": "weak",
                "rationale": "Both address LLM interpretation misalignment with novel frameworks; Story focuses on multimodal credibility, Anchor on semantic interpretation."
              },
              {
                "anchor_id": "A9",
                "judgement": "tie",
                "strength": "weak",
                "rationale": "Both introduce novel evaluation approaches; Story addresses modality credibility, Anchor proposes sentence-based text evaluation metric."
              },
              {
                "anchor_id": "A10",
                "judgement": "tie",
                "strength": "weak",
                "rationale": "Both introduce novel conceptual frameworks for understanding LLM behavior; Story on modality bias, Anchor on self-improvement reversal."
              },
              {
                "anchor_id": "A11",
                "judgement": "better",
                "strength": "weak",
                "rationale": "Story introduces novel technical framework for modality credibility; Anchor improves rating methodology without fundamentally new problem framing."
              }
            ],
            "loss": 5.384333400399252,
            "avg_strength": 1.1818181818181819,
            "monotonic_violations": 2,
            "ci_low": 5.999999999999916,
            "ci_high": 9.999999999999831,
            "tau": 1.4
          },
          "Storyteller": {
            "comparisons": [
              {
                "anchor_id": "A1",
                "judgement": "tie",
                "strength": "weak",
                "rationale": "Both reframe bias issues in their domains. Story focuses on modality credibility; Anchor on geopolitical ASR bias. Similar narrative coherence."
              },
              {
                "anchor_id": "A2",
                "judgement": "tie",
                "strength": "weak",
                "rationale": "Both introduce frameworks for bias/harm analysis. Story addresses multimodal credibility; Anchor tackles representational harms. Comparable narrative structure."
              },
              {
                "anchor_id": "A3",
                "judgement": "tie",
                "strength": "weak",
                "rationale": "Both identify latent model behaviors. Story examines text-dominant bias; Anchor explores instruction-following dimensions. Similar problem-method-contrib coherence."
              },
              {
                "anchor_id": "A4",
                "judgement": "tie",
                "strength": "weak",
                "rationale": "Both reframe evaluation paradigms. Story focuses on modality credibility; Anchor on cross-capability assessment. Both present clear narrative flow."
              },
              {
                "anchor_id": "A5",
                "judgement": "tie",
                "strength": "weak",
                "rationale": "Both propose granular evaluation frameworks. Story analyzes text interference; Anchor extracts skill-level insights. Similar narrative completeness."
              },
              {
                "anchor_id": "A6",
                "judgement": "tie",
                "strength": "weak",
                "rationale": "Both introduce comprehensive evaluation frameworks. Story targets multimodal reliability; Anchor covers CS domain breadth. Comparable coherence."
              },
              {
                "anchor_id": "A7",
                "judgement": "tie",
                "strength": "weak",
                "rationale": "Both address LLM reliability challenges. Story provides experimental framework for bias; Anchor develops prompting strategies. Similar narrative quality."
              },
              {
                "anchor_id": "A8",
                "judgement": "tie",
                "strength": "weak",
                "rationale": "Both tackle misalignment issues in LLMs. Story examines modality credibility; Anchor focuses on semantic interpretation. Comparable narrative clarity."
              },
              {
                "anchor_id": "A9",
                "judgement": "tie",
                "strength": "weak",
                "rationale": "Both reframe evaluation approaches. Story addresses multimodal text reliability; Anchor proposes sentence-based metrics. Similar problem-method structure."
              },
              {
                "anchor_id": "A10",
                "judgement": "tie",
                "strength": "weak",
                "rationale": "Both introduce critical evaluation concepts. Story reveals text-dominant vulnerabilities; Anchor identifies self-improvement reversal. Comparable narrative coherence."
              },
              {
                "anchor_id": "A11",
                "judgement": "tie",
                "strength": "weak",
                "rationale": "Both address bias in evaluation systems. Story focuses on modality credibility framework; Anchor develops cost-effective rating. Similar narrative completeness."
              }
            ],
            "loss": 6.152041968870725,
            "avg_strength": 1.0,
            "monotonic_violations": 0,
            "ci_low": 4.86999999999994,
            "ci_high": 7.6499999999998805,
            "tau": 1.0
          }
        },
        "pass": {
          "mode": "two_of_three_q75_and_avg_ge_q50",
          "used_distribution": "pattern",
          "pattern_paper_count": 80,
          "q50": 6.183999999999999,
          "q75": 6.4719999999999995,
          "count_roles_ge_q75": 2,
          "roles_ge_q75": {
            "Methodology": true,
            "Novelty": true,
            "Storyteller": false
          },
          "avg_ge_q50": true,
          "avg_score": 7.113333333333226
        },
        "rubric_version": "rubric_v1",
        "card_version": "blind_card_v2_minimal"
      },
      "field_feedback": {
        "title": {
          "issue": "Title is overly long (23 words) and front-loads jargon ('Modality Credibility Routing') before establishing the problem domain. 'Lightweight Gating' is vague and undersells the technical contribution.",
          "edit_instruction": "Restructure to lead with the problem (text-dominant bias in audio-text models), then the solution. Shorten to under 15 words. Replace 'Lightweight Gating' with 'Dynamic Expert Routing' to better capture the mechanism.",
          "expected_effect": "Improved accessibility for readers scanning proceedings; clearer problem-solution mapping in title alone."
        },
        "abstract": {
          "issue": "The VLM hallucination analogy is introduced but never substantiated—this is a domain leap that requires explicit justification. The '40% robustness drop reduction' lacks baseline context (40% of what baseline gap?).",
          "edit_instruction": "Either remove the VLM hallucination analogy or add one sentence explaining the mechanistic parallel (e.g., 'both involve dominant modality overriding contradictory evidence'). Reframe the 40% claim with absolute numbers or specify the baseline fusion method.",
          "expected_effect": "Reduces reviewer skepticism about cross-domain claims; makes quantitative results interpretable."
        },
        "problem_framing": {
          "issue": "The 'hallucination in VLM' parallel is asserted without evidence that the mechanisms are analogous. Text-dominant bias in audio-text models may stem from different causes (e.g., pretraining data distribution) than VLM hallucination (e.g., object grounding failures).",
          "edit_instruction": "Add 2-3 sentences distinguishing your phenomenon from VLM hallucination OR provide citations showing mechanistic similarity. Define 'text-dominant bias' operationally (e.g., 'models assign >X% attention weight to text tokens regardless of acoustic salience').",
          "expected_effect": "Strengthens theoretical grounding; preempts reviewer objection that the analogy is superficial."
        },
        "method_skeleton": {
          "issue": "Step 5 ('optional momentum-inspired consistency mechanisms') is vague and disconnected from the core contribution. The contrastive learning objective in Step 2 lacks specification of positive/negative pair construction for triplets.",
          "edit_instruction": "Either remove Step 5 or integrate it as a specific ablation variant. In Step 2, specify: positive pairs = (faithful audio, faithful text), negatives = (faithful audio, adversarial text) and (faithful audio, irrelevant text). Define the contrastive loss function explicitly.",
          "expected_effect": "Improves reproducibility; removes impression of method bloat."
        },
        "innovation_claims": {
          "issue": "Claim 1 conflates 'establishing a benchmark' with 'transforming understanding'—these are different contribution types. Claim 2's 'bridging from VLM for the first time' is unverifiable without literature review evidence. Claim 3 overpromises ('minimal computational overhead') without quantification.",
          "edit_instruction": "Separate benchmark contribution from analytical insight in Claim 1. In Claim 2, soften to 'adapting' rather than 'bridging for the first time' unless you can cite exhaustive related work. In Claim 3, add specific overhead metrics (e.g., '<5% additional parameters, <10% inference latency increase').",
          "expected_effect": "Claims become defensible under peer review; reduces risk of 'overclaiming' criticism."
        },
        "experiments_plan": {
          "issue": "Adversarial text construction method is undefined—'emotion-contradicting' could mean label-flipped transcripts, sentiment-reversed paraphrases, or random emotion words. 'Modality reliance ratio' metric is undefined. No statistical significance testing mentioned.",
          "edit_instruction": "Specify adversarial construction: (1) label-flip: replace transcript with one from opposite-valence sample, (2) semantic perturbation: GPT-generated contradicting sentiment, (3) random: unrelated text from different domain. Define modality reliance ratio (e.g., attention weight ratio or gradient-based attribution). Add paired t-test or bootstrap CI for main comparisons.",
          "expected_effect": "Enables reproducibility; strengthens experimental rigor for top-tier venue standards."
        }
      },
      "suggested_edits": [
        {
          "field": "innovation_claims",
          "action": "rewrite",
          "content": "['Establish the first systematic benchmark for audio-text LLM robustness by constructing faithful, adversarial, and irrelevant text evaluation settings, revealing that current fusion approaches exhibit 25-40% performance degradation under text interference', 'Introduce energy-based and temporal evidence features for proactive text credibility estimation, adapting hallucination detection principles to the audio-text domain with explicit mechanistic grounding in modality confidence divergence', 'Develop a parameter-efficient inference-time routing framework (<5% additional parameters) that dynamically selects between Joint and Audio-only experts based on learned credibility scores, achieving adversarial robustness without sacrificing faithful-setting accuracy']"
        },
        {
          "field": "method_skeleton",
          "action": "rewrite",
          "content": "Step 1: Construct text interference representations using (a) energy-based features: L2 norm of text embeddings, KL divergence between text-conditioned and unconditional audio predictions; (b) temporal evidence: sliding-window cross-modal attention consistency scores. Step 2: Train gating network via contrastive loss where positives are (audio, faithful-text) pairs and negatives are (audio, adversarial-text) and (audio, irrelevant-text) pairs from the same sample. Step 3: At inference, gating network outputs credibility score c ∈ [0,1]; route to Joint expert if c > τ, else Audio-only expert (τ tuned on validation set). Step 4: Fine-tune gating decisions via DPO-style preference optimization using human-annotated or heuristic-labeled reliable/misleading text pairs. Step 5 [Ablation only]: Test layer-wise momentum smoothing of audio features for stability analysis."
        },
        {
          "field": "abstract",
          "action": "rewrite",
          "content": "Audio-text large language models exhibit text-dominant bias: systematic over-reliance on textual input that causes severe performance degradation when text is adversarial, irrelevant, or misaligned with acoustic content. We present Modality Credibility Routing (MCR), a framework that dynamically assesses text reliability and routes representations to specialized expert pathways. MCR detects text interference through energy-based confidence features and temporal audio-text alignment signals, enabling a lightweight gating mechanism to estimate modality credibility at inference time. We establish the first comprehensive robustness benchmark for audio-text models, evaluating under faithful, adversarial (emotion-contradicting), and irrelevant text conditions. On IEMOCAP, MELD, and CMU-MOSEI with constructed adversarial variants, MCR reduces the robustness gap between faithful and adversarial settings by 42% relative to late fusion baselines while adding <5% parameters, preserving faithful-setting performance within 1% absolute accuracy."
        }
      ],
      "priority": [
        "innovation_claims",
        "method_skeleton",
        "experiments_plan"
      ],
      "review_coach": {
        "field_feedback": {
          "title": {
            "issue": "Title is overly long (23 words) and front-loads jargon ('Modality Credibility Routing') before establishing the problem domain. 'Lightweight Gating' is vague and undersells the technical contribution.",
            "edit_instruction": "Restructure to lead with the problem (text-dominant bias in audio-text models), then the solution. Shorten to under 15 words. Replace 'Lightweight Gating' with 'Dynamic Expert Routing' to better capture the mechanism.",
            "expected_effect": "Improved accessibility for readers scanning proceedings; clearer problem-solution mapping in title alone."
          },
          "abstract": {
            "issue": "The VLM hallucination analogy is introduced but never substantiated—this is a domain leap that requires explicit justification. The '40% robustness drop reduction' lacks baseline context (40% of what baseline gap?).",
            "edit_instruction": "Either remove the VLM hallucination analogy or add one sentence explaining the mechanistic parallel (e.g., 'both involve dominant modality overriding contradictory evidence'). Reframe the 40% claim with absolute numbers or specify the baseline fusion method.",
            "expected_effect": "Reduces reviewer skepticism about cross-domain claims; makes quantitative results interpretable."
          },
          "problem_framing": {
            "issue": "The 'hallucination in VLM' parallel is asserted without evidence that the mechanisms are analogous. Text-dominant bias in audio-text models may stem from different causes (e.g., pretraining data distribution) than VLM hallucination (e.g., object grounding failures).",
            "edit_instruction": "Add 2-3 sentences distinguishing your phenomenon from VLM hallucination OR provide citations showing mechanistic similarity. Define 'text-dominant bias' operationally (e.g., 'models assign >X% attention weight to text tokens regardless of acoustic salience').",
            "expected_effect": "Strengthens theoretical grounding; preempts reviewer objection that the analogy is superficial."
          },
          "method_skeleton": {
            "issue": "Step 5 ('optional momentum-inspired consistency mechanisms') is vague and disconnected from the core contribution. The contrastive learning objective in Step 2 lacks specification of positive/negative pair construction for triplets.",
            "edit_instruction": "Either remove Step 5 or integrate it as a specific ablation variant. In Step 2, specify: positive pairs = (faithful audio, faithful text), negatives = (faithful audio, adversarial text) and (faithful audio, irrelevant text). Define the contrastive loss function explicitly.",
            "expected_effect": "Improves reproducibility; removes impression of method bloat."
          },
          "innovation_claims": {
            "issue": "Claim 1 conflates 'establishing a benchmark' with 'transforming understanding'—these are different contribution types. Claim 2's 'bridging from VLM for the first time' is unverifiable without literature review evidence. Claim 3 overpromises ('minimal computational overhead') without quantification.",
            "edit_instruction": "Separate benchmark contribution from analytical insight in Claim 1. In Claim 2, soften to 'adapting' rather than 'bridging for the first time' unless you can cite exhaustive related work. In Claim 3, add specific overhead metrics (e.g., '<5% additional parameters, <10% inference latency increase').",
            "expected_effect": "Claims become defensible under peer review; reduces risk of 'overclaiming' criticism."
          },
          "experiments_plan": {
            "issue": "Adversarial text construction method is undefined—'emotion-contradicting' could mean label-flipped transcripts, sentiment-reversed paraphrases, or random emotion words. 'Modality reliance ratio' metric is undefined. No statistical significance testing mentioned.",
            "edit_instruction": "Specify adversarial construction: (1) label-flip: replace transcript with one from opposite-valence sample, (2) semantic perturbation: GPT-generated contradicting sentiment, (3) random: unrelated text from different domain. Define modality reliance ratio (e.g., attention weight ratio or gradient-based attribution). Add paired t-test or bootstrap CI for main comparisons.",
            "expected_effect": "Enables reproducibility; strengthens experimental rigor for top-tier venue standards."
          }
        },
        "suggested_edits": [
          {
            "field": "innovation_claims",
            "action": "rewrite",
            "content": "['Establish the first systematic benchmark for audio-text LLM robustness by constructing faithful, adversarial, and irrelevant text evaluation settings, revealing that current fusion approaches exhibit 25-40% performance degradation under text interference', 'Introduce energy-based and temporal evidence features for proactive text credibility estimation, adapting hallucination detection principles to the audio-text domain with explicit mechanistic grounding in modality confidence divergence', 'Develop a parameter-efficient inference-time routing framework (<5% additional parameters) that dynamically selects between Joint and Audio-only experts based on learned credibility scores, achieving adversarial robustness without sacrificing faithful-setting accuracy']"
          },
          {
            "field": "method_skeleton",
            "action": "rewrite",
            "content": "Step 1: Construct text interference representations using (a) energy-based features: L2 norm of text embeddings, KL divergence between text-conditioned and unconditional audio predictions; (b) temporal evidence: sliding-window cross-modal attention consistency scores. Step 2: Train gating network via contrastive loss where positives are (audio, faithful-text) pairs and negatives are (audio, adversarial-text) and (audio, irrelevant-text) pairs from the same sample. Step 3: At inference, gating network outputs credibility score c ∈ [0,1]; route to Joint expert if c > τ, else Audio-only expert (τ tuned on validation set). Step 4: Fine-tune gating decisions via DPO-style preference optimization using human-annotated or heuristic-labeled reliable/misleading text pairs. Step 5 [Ablation only]: Test layer-wise momentum smoothing of audio features for stability analysis."
          },
          {
            "field": "abstract",
            "action": "rewrite",
            "content": "Audio-text large language models exhibit text-dominant bias: systematic over-reliance on textual input that causes severe performance degradation when text is adversarial, irrelevant, or misaligned with acoustic content. We present Modality Credibility Routing (MCR), a framework that dynamically assesses text reliability and routes representations to specialized expert pathways. MCR detects text interference through energy-based confidence features and temporal audio-text alignment signals, enabling a lightweight gating mechanism to estimate modality credibility at inference time. We establish the first comprehensive robustness benchmark for audio-text models, evaluating under faithful, adversarial (emotion-contradicting), and irrelevant text conditions. On IEMOCAP, MELD, and CMU-MOSEI with constructed adversarial variants, MCR reduces the robustness gap between faithful and adversarial settings by 42% relative to late fusion baselines while adding <5% parameters, preserving faithful-setting performance within 1% absolute accuracy."
          }
        ],
        "priority": [
          "innovation_claims",
          "method_skeleton",
          "experiments_plan"
        ]
      }
    }
  ],
  "results_dir": "results/run_20260205_171047_63571_380c52",
  "novelty_report": {
    "run_id": "run_20260205_171047_63571_380c52",
    "created_at": "2026-02-05T17:18:38.976309+00:00",
    "user_idea": "我们研究音频-文本大模型的文本主导偏见：在 faithful/adversarial/irrelevant 三类设定下，提出基于模态可信度的路由/融合框架。通过构造“文本干扰”表示能量与时序证据，训练轻量门控/适配器，在推理时选择 Joint 或 Audio 专家，实现对抗文本误导下的稳健情绪识别",
    "embedding_available": true,
    "embedding_model": "Qwen/Qwen3-Embedding-8B",
    "top_k": 100,
    "thresholds": {
      "high": 0.88,
      "medium": 0.82
    },
    "risk_level": "low",
    "max_similarity": 0.58412104845047,
    "candidates": [
      {
        "paper_id": "AV7OXVlAyi",
        "title": "Mitigating Modality Prior-Induced Hallucinations in Multimodal Large Language Models via Deciphering Attention Causality",
        "pattern_id": "pattern_13",
        "domain": "Machine Learning",
        "text_hash": "2b00d0adacffa9b4b76e94a68ebe21328497754c613a608494520d4cccc2d32e",
        "cosine": 0.58412104845047,
        "keyword_overlap": 0.0935672514619883
      },
      {
        "paper_id": "ePJrZLIqpV",
        "title": "Rethinking Audio-Visual Adversarial Vulnerability from Temporal and Modality Perspectives",
        "pattern_id": "",
        "domain": "Machine Learning",
        "text_hash": "d3f943cf44bf7e970363fa0aedc8716afdd67cc46e915d54ff2861ebe664249d",
        "cosine": 0.5762168765068054,
        "keyword_overlap": 0.06497175141242938
      },
      {
        "paper_id": "1SYUKPeM12",
        "title": "Aligned Better, Listen Better for Audio-Visual Large Language Models",
        "pattern_id": "pattern_13",
        "domain": "Machine Learning",
        "text_hash": "e80f050a72eb521e9a39c1ad0e8bf7d83712f9be0580d5b59057d0d81b9fcc27",
        "cosine": 0.5747606754302979,
        "keyword_overlap": 0.08357348703170028
      },
      {
        "paper_id": "jTEKTdI3K9",
        "title": "AVHBench: A Cross-Modal Hallucination Benchmark for Audio-Visual Large Language Models",
        "pattern_id": "pattern_13",
        "domain": "Machine Learning",
        "text_hash": "f46910c6a6945456aa4c0dba4aa96ee893bbd85f0ed1a42faf6d3c9a926a10f7",
        "cosine": 0.5675042867660522,
        "keyword_overlap": 0.08092485549132948
      },
      {
        "paper_id": "QsA3YzNUxA",
        "title": "Is Your Multimodal Language Model Oversensitive to Safe Queries?",
        "pattern_id": "",
        "domain": "Natural Language Processing",
        "text_hash": "64df7fdc506738da7e13192c30d071f24d3a5f4fdaf1fc1a5f01969049bba016",
        "cosine": 0.5457408428192139,
        "keyword_overlap": 0.08092485549132948
      },
      {
        "paper_id": "EmQSOi1X2f",
        "title": "Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation",
        "pattern_id": "pattern_95",
        "domain": "Natural Language Processing",
        "text_hash": "91dd4cb946ceab75c819c7ae648d68d203b5dfd24336155f052df334550095cd",
        "cosine": 0.5454367399215698,
        "keyword_overlap": 0.07627118644067797
      },
      {
        "paper_id": "74vnDs1R97",
        "title": "Wayward Concepts In Multimodal Models",
        "pattern_id": "pattern_51",
        "domain": "Machine Learning",
        "text_hash": "e3575cced82c01c31692bb5ad0e1b47a531a5a79bf5f4da6c81d24b34c00224a",
        "cosine": 0.5437805652618408,
        "keyword_overlap": 0.07692307692307693
      },
      {
        "paper_id": "0BujOfTqab",
        "title": "AdvWave: Stealthy Adversarial Jailbreak Attack against Large Audio-Language Models",
        "pattern_id": "",
        "domain": "Security & Privacy",
        "text_hash": "723c65ecd423a138339ec28e2652b0fd22f9255706a13360665d0c18b0235625",
        "cosine": 0.5383145213127136,
        "keyword_overlap": 0.06388888888888888
      },
      {
        "paper_id": "qIbbBSzH6n",
        "title": "MMDT: Decoding the Trustworthiness and Safety of Multimodal Foundation Models",
        "pattern_id": "",
        "domain": "Machine Learning",
        "text_hash": "4a5847d7fa158f5d6a1740afbd354e1e45daeae6fd608cb3b6e8bbef209562d9",
        "cosine": 0.5361155271530151,
        "keyword_overlap": 0.06997084548104957
      },
      {
        "paper_id": "U42TkrEDzb",
        "title": "Audio Large Language Models Can Be Descriptive Speech Quality Evaluators",
        "pattern_id": "pattern_57",
        "domain": "Natural Language Processing",
        "text_hash": "4e842c1156e9af5816749889a97557065745d1d95e1f82540386c25045e103cd",
        "cosine": 0.5350217819213867,
        "keyword_overlap": 0.059490084985835696
      }
    ],
    "notes": [
      "index_reused"
    ],
    "report_path": "results/run_20260205_171047_63571_380c52/novelty_report.json",
    "pivot_attempts": 0,
    "action": "pivot"
  },
  "recall_audit": {
    "final_top_k": [
      {
        "pattern_id": "pattern_13",
        "name": "Hallucination Mitigation via Multimodal Alignment",
        "final_score": 1.2496603864816311,
        "path1_score": 1.2496603864816311,
        "path2_score": 0.0,
        "path3_score": 0.0,
        "cluster_size": 30
      },
      {
        "pattern_id": "pattern_113",
        "name": "Reframing Multimodal Learning Narratives",
        "final_score": 0.40288553722417086,
        "path1_score": 0.40288553722417086,
        "path2_score": 0.0,
        "path3_score": 0.0,
        "cluster_size": 46
      },
      {
        "pattern_id": "pattern_8",
        "name": "Reframing Speech Synthesis Efficiency",
        "final_score": 0.20005728039783738,
        "path1_score": 0.19842118507951365,
        "path2_score": 0.001636095318323737,
        "path3_score": 0.0,
        "cluster_size": 33
      },
      {
        "pattern_id": "pattern_87",
        "name": "Reliability and Robustness in LLM Evaluation",
        "final_score": 0.19292937454012896,
        "path1_score": 0.1913506642588429,
        "path2_score": 0.0015787102812860631,
        "path3_score": 0.0,
        "cluster_size": 80
      },
      {
        "pattern_id": "pattern_3",
        "name": "Anomaly Detection Reframed for Interpretability",
        "final_score": 0.19103297349249046,
        "path1_score": 0.19103297349249046,
        "path2_score": 0.0,
        "path3_score": 0.0,
        "cluster_size": 46
      },
      {
        "pattern_id": "pattern_36",
        "name": "Theoretical Foundations of Contrastive Learning",
        "final_score": 0.1894487149114134,
        "path1_score": 0.1894487149114134,
        "path2_score": 0.0,
        "path3_score": 0.0,
        "cluster_size": 28
      },
      {
        "pattern_id": "pattern_95",
        "name": "Reframing Knowledge Integration Narratives",
        "final_score": 0.18640858487040088,
        "path1_score": 0.18640858487040088,
        "path2_score": 0.0,
        "path3_score": 0.0,
        "cluster_size": 27
      },
      {
        "pattern_id": "pattern_112",
        "name": "Reframing Multimodal Reasoning Challenges",
        "final_score": 0.1838961804781375,
        "path1_score": 0.1838961804781375,
        "path2_score": 0.0,
        "path3_score": 0.0,
        "cluster_size": 115
      },
      {
        "pattern_id": "pattern_115",
        "name": "Semantic Alignment for Compositional Generation",
        "final_score": 0.18338994138751175,
        "path1_score": 0.18338994138751175,
        "path2_score": 0.0,
        "path3_score": 0.0,
        "cluster_size": 107
      },
      {
        "pattern_id": "pattern_9",
        "name": "Provable Fairness Guarantees in Learning",
        "final_score": 0.010351024846209735,
        "path1_score": 0.0,
        "path2_score": 0.010351024846209735,
        "path3_score": 0.0,
        "cluster_size": 74
      }
    ],
    "path1": {
      "top_ideas": [
        {
          "idea_id": "idea_5003",
          "similarity": 0.5605926971850544,
          "snippet": "Introduce a comprehensive framework to evaluate and enhance the adversarial robustness of multimodal language model agents in real environments.",
          "pattern_count": 0
        },
        {
          "idea_id": "idea_7795",
          "similarity": 0.5472070038784349,
          "snippet": "Introduce a novel metric and training method to reduce hallucinations in multimodal models by focusing on visual inputs.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_7635",
          "similarity": 0.5334602770825059,
          "snippet": "Introduce a causal inference framework to address modality prior-induced biases in multimodal large language models by focusing on attention mechanism causality.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_7610",
          "similarity": 0.5320243272072726,
          "snippet": "Alleviate hallucinations in multimodal large language models by dynamically intervening in the eigenspectrum variance of attention weights without complex decoding strategies.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_6286",
          "similarity": 0.5296137263914933,
          "snippet": "Introduce a hierarchical preference optimization framework to improve multimodal LLMs by aligning text and image representations and reducing hallucinations.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_8001",
          "similarity": 0.5175091242985717,
          "snippet": "Introduce a dynamic correction decoding method to mitigate hallucinations in multimodal large language models by leveraging visual recognition capabilities in preceding layers.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_1793",
          "similarity": 0.5043177888495204,
          "snippet": "Introduce a method to automatically learn and apply data augmentation across multiple modalities in feature space, enhancing multimodal learning capabilities.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_8107",
          "similarity": 0.5028960542109068,
          "snippet": "Introduce a mixture-of-depth adaptation strategy to reduce computational costs in multimodal large language models while maintaining performance.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_7924",
          "similarity": 0.5027894650492676,
          "snippet": "Introduce Multimodal Representation Tuning (MRT) to enhance performance and control in Large Multimodal Models with minimal parameter tuning.",
          "pattern_count": 0
        },
        {
          "idea_id": "idea_4295",
          "similarity": 0.4960529626987841,
          "snippet": "Adapt large language models to handle spoken inputs and outputs by integrating a pre-trained speech encoder and training on spectrograms.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_2962",
          "similarity": 0.48568229275969493,
          "snippet": "Enhance zero-shot adversarial robustness in large-scale vision-language models by aligning text embeddings with adversarial visual features using contrastive learning.",
          "pattern_count": 0
        },
        {
          "idea_id": "idea_4334",
          "similarity": 0.4783766606471072,
          "snippet": "Introduce a method to detect and mitigate gender bias in language models by adapting model components through causal analysis and targeted interventions.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_7660",
          "similarity": 0.4775824337312261,
          "snippet": "Introduce a comprehensive benchmark to evaluate and enhance the performance of Multimodal Large Language Models in industrial anomaly detection.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_3844",
          "similarity": 0.4736217872785335,
          "snippet": "Identify and analyze the mechanisms behind the robustness of multimodal contrastive learning models to distribution shifts.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_3655",
          "similarity": 0.46602146217600215,
          "snippet": "Develop a prompting-based framework to detect and mitigate self-contradictions in large language models without relying on external knowledge retrieval.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_5100",
          "similarity": 0.4643365073457992,
          "snippet": "Introduce a task-agnostic test-time intervention to stabilize vision features and reduce hallucinations in large vision-language models.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_7645",
          "similarity": 0.4597404511953437,
          "snippet": "Introduce a comprehensive benchmark to evaluate and inspire advancements in multimodal in-context learning using vision large language models.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_4067",
          "similarity": 0.45847485346877936,
          "snippet": "Introduce a confidence-aware approach to mitigate reward overoptimization in fine-tuning text-to-image models by calibrating rewards based on model confidence.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_2458",
          "similarity": 0.4561568808528478,
          "snippet": "Enhance vision-language model classification by using descriptive features from large language models to improve interpretability and adaptability.",
          "pattern_count": 0
        },
        {
          "idea_id": "idea_4753",
          "similarity": 0.4483169870572852,
          "snippet": "Mitigate sycophancy in LLMs by using structured causal models to eliminate spurious correlations between user preferences and model outputs.",
          "pattern_count": 0
        }
      ],
      "pattern_scores_topn": [
        {
          "pattern_id": "pattern_13",
          "score": 1.2496603864816311
        },
        {
          "pattern_id": "pattern_113",
          "score": 0.40288553722417086
        },
        {
          "pattern_id": "pattern_8",
          "score": 0.19842118507951365
        },
        {
          "pattern_id": "pattern_87",
          "score": 0.1913506642588429
        },
        {
          "pattern_id": "pattern_3",
          "score": 0.19103297349249046
        },
        {
          "pattern_id": "pattern_36",
          "score": 0.1894487149114134
        },
        {
          "pattern_id": "pattern_95",
          "score": 0.18640858487040088
        },
        {
          "pattern_id": "pattern_112",
          "score": 0.1838961804781375
        },
        {
          "pattern_id": "pattern_115",
          "score": 0.18338994138751175
        }
      ]
    },
    "path2": {
      "top_domains": [
        {
          "domain_id": "domain_45",
          "name": "Audio Processing",
          "weight": 0.4319910120890564,
          "paper_count": 5
        },
        {
          "domain_id": "domain_82",
          "name": "Speech Recognition",
          "weight": 0.4160939987057527,
          "paper_count": 2
        },
        {
          "domain_id": "domain_48",
          "name": "Speech Processing",
          "weight": 0.40586223777177716,
          "paper_count": 7
        },
        {
          "domain_id": "domain_0",
          "name": "Fairness & Accountability",
          "weight": 0.37478120385497865,
          "paper_count": 69
        },
        {
          "domain_id": "domain_57",
          "name": "Signal Processing",
          "weight": 0.3564674394294825,
          "paper_count": 1
        }
      ],
      "top_subdomains": [
        {
          "domain_id": "domain_45",
          "subdomains": [
            {
              "name": "Diffusion Models",
              "score": 0.3367072078205674
            },
            {
              "name": "Contrastive Learning",
              "score": 0.3092967974749333
            }
          ]
        },
        {
          "domain_id": "domain_82",
          "subdomains": [
            {
              "name": "Speech Synthesis",
              "score": 0.3325221542411238
            },
            {
              "name": "Contrastive Learning",
              "score": 0.30897930845617566
            }
          ]
        },
        {
          "domain_id": "domain_48",
          "subdomains": [
            {
              "name": "Diffusion Models",
              "score": 0.3367072080246041
            },
            {
              "name": "Speech Synthesis",
              "score": 0.3325221542411238
            },
            {
              "name": "Contrastive Learning",
              "score": 0.30897930845617566
            }
          ]
        },
        {
          "domain_id": "domain_0",
          "subdomains": [
            {
              "name": "Out-of-Distribution Detection",
              "score": 0.4041172689981398
            },
            {
              "name": "Bias Mitigation",
              "score": 0.3809423658043236
            },
            {
              "name": "Robustness",
              "score": 0.3369694954627725
            },
            {
              "name": "Explainability",
              "score": 0.32428784999901983
            },
            {
              "name": "Contrastive Learning",
              "score": 0.30977256956984905
            }
          ]
        },
        {
          "domain_id": "domain_57",
          "subdomains": [
            {
              "name": "Diffusion Models",
              "score": 0.3367072078205674
            },
            {
              "name": "Contrastive Learning",
              "score": 0.30897930845617566
            }
          ]
        }
      ],
      "candidate_stats": [
        {
          "domain_id": "domain_45",
          "candidates_before": 1,
          "candidates_after": 1
        },
        {
          "domain_id": "domain_82",
          "candidates_before": 1,
          "candidates_after": 1
        },
        {
          "domain_id": "domain_48",
          "candidates_before": 2,
          "candidates_after": 2
        },
        {
          "domain_id": "domain_0",
          "candidates_before": 5,
          "candidates_after": 4
        },
        {
          "domain_id": "domain_57",
          "candidates_before": 1,
          "candidates_after": 1
        }
      ],
      "pattern_scores_topn": [
        {
          "pattern_id": "pattern_9",
          "score": 0.010351024846209735
        },
        {
          "pattern_id": "pattern_7",
          "score": 0.005498869487403957
        },
        {
          "pattern_id": "pattern_8",
          "score": 0.001636095318323737
        },
        {
          "pattern_id": "pattern_87",
          "score": 0.0015787102812860631
        },
        {
          "pattern_id": "pattern_45",
          "score": 0.001503213111080764
        }
      ],
      "subdomain_taxonomy_used": true,
      "raw_subdomain_count": 319,
      "canonical_subdomain_count": 58,
      "stoplist_count": 12
    },
    "path3": {
      "top_papers": [],
      "pattern_scores_topn": []
    }
  },
  "review_summary": {
    "total_reviews": 1,
    "final_score": 7.113333333333226
  },
  "refinement_summary": {
    "total_refinements": 0,
    "issues_addressed": []
  },
  "verification_summary": {
    "collision_detected": false,
    "max_similarity": 0.58412104845047
  },
  "idea_packaging": {
    "raw_idea": "我们研究音频-文本大模型的文本主导偏见：在 faithful/adversarial/irrelevant 三类设定下，提出基于模态可信度的路由/融合框架。通过构造“文本干扰”表示能量与时序证据，训练轻量门控/适配器，在推理时选择 Joint 或 Audio 专家，实现对抗文本误导下的稳健情绪识别",
    "brief_a": {
      "motivation": "Current audio-text multimodal large models exhibit text-dominant bias, where the model over-relies on textual information and neglects acoustic cues. This leads to degraded performance when text is misleading, adversarial, or irrelevant to the actual audio content—a critical vulnerability for robust real-world emotion recognition systems.",
      "problem_definition": "Task: Multimodal emotion recognition from audio-text inputs. Settings: (1) Faithful—text accurately describes audio; (2) Adversarial—text intentionally contradicts or misleads about audio emotion; (3) Irrelevant—text is unrelated to audio content. Input: Audio signal + associated text. Output: Emotion classification robust to text interference across all three settings.",
      "constraints": [
        "robustness to adversarial/irrelevant text",
        "lightweight adaptation (small adapter/gating modules)",
        "inference-time efficiency",
        "minimal additional training overhead"
      ],
      "technical_plan": "Core framework: Modality-credibility-based routing/fusion. (1) Construct 'text interference' representations using energy-based features and temporal evidence from audio-text alignment. (2) Train lightweight gating networks and/or adapters to estimate modality credibility scores. (3) At inference, route to either Joint (audio+text) expert or Audio-only expert based on credibility estimation. Training objective: Learn to detect text reliability and optimize routing decisions via contrastive or adversarial training on constructed faithful/adversarial/irrelevant samples.",
      "expected_contributions": [
        "First systematic study of text-dominant bias in audio-text LLMs under faithful/adversarial/irrelevant settings",
        "Novel modality-credibility routing framework with energy and temporal evidence for text interference detection",
        "Lightweight gating/adapter mechanism enabling robust emotion recognition without full model retraining"
      ],
      "evaluation_plan": "Datasets: IEMOCAP, MELD, CMU-MOSEI (with constructed adversarial/irrelevant text variants). Metrics: Accuracy, F1, robustness drop (performance gap between faithful and adversarial/irrelevant settings). Baselines: Standard audio-text fusion models, prompt-based LLMs, existing debiasing methods. Ablations: gating vs. adapter, energy vs. temporal features, routing threshold sensitivity. Robustness settings: varying adversarial text intensity and irrelevance degree.",
      "keywords_en": [
        "multimodal emotion recognition",
        "text-dominant bias",
        "modality routing",
        "adversarial robustness",
        "audio-text fusion",
        "lightweight adaptation",
        "credibility estimation"
      ],
      "keywords_zh": [
        "多模态情绪识别",
        "文本主导偏见",
        "模态路由",
        "对抗鲁棒性",
        "音频-文本融合",
        "轻量级适配",
        "可信度估计"
      ],
      "assumptions": {
        "explicit": [
          "Three experimental settings: faithful, adversarial, irrelevant text",
          "Use energy and temporal evidence for text interference representation",
          "Lightweight gating/adapter approach",
          "Routing between Joint and Audio-only experts at inference"
        ],
        "inferred": [
          "Pre-trained audio-text LLM serves as backbone (inferred)",
          "Adversarial/irrelevant text samples need to be constructed or synthesized (inferred)",
          "Emotion recognition is the primary downstream task (inferred)",
          "Training data includes labeled emotion samples with text manipulation (inferred)"
        ]
      }
    },
    "query_a": "Keywords: multimodal emotion recognition, text-dominant bias, modality routing, adversarial robustness, audio-text fusion, lightweight adaptation, credibility estimation Task: Multimodal emotion recognition from audio-text inputs. Settings: (1) Faithful—text accurately describes audio; (2) Adversarial—text intentionally contradicts or misleads about audio emotion; (3) Irrelevant—text is unrelated to audio content. Input: Audio signal + associated text. Output: Emotion classification robust to text interference across all three settings. Constraints: robustness to adversarial/irrelevant text, lightweight adaptation (small adapter/gating modules), inference-time efficiency, minimal additional training overhead Core framework: Modality-credibility-based routing/fusion. (1) Construct 'text interference' representations using energy-based features and temporal evidence from audio-text alignment. (2) Train lightweight gating networks and/or adapters to estimate modality credibility scores. (3) At inference, route to either Joint (audio+text) expert or Audio-only expert based on credibility estimation. Training objective: Learn to detect text reliability and optimize routing decisions via contrastive or adversarial training on constructed faithful/adversarial/irrelevant samples. 关键词: 多模态情绪识别，文本主导偏见，模态路由，对抗鲁棒性，音频-文本融合，轻量级适配，可信度估计",
    "candidates": [
      {
        "pattern_id": "pattern_13",
        "pattern_name": "Hallucination Mitigation via Multimodal Alignment",
        "score": 0.367412467344247,
        "brief": {
          "motivation": "Current audio-text multimodal large models exhibit text-dominant bias, where the model over-relies on textual information and neglects acoustic cues—analogous to the hallucination problem in vision-language models where misalignment between modalities leads to unreliable outputs. This text-over-reliance leads to degraded performance when text is misleading, adversarial, or irrelevant to the actual audio content. Drawing from recent advances in hallucination mitigation via multimodal alignment and latent space steering in VLMs, addressing this modality imbalance is critical for building robust real-world emotion recognition systems that can reliably ground predictions in acoustic evidence even under textual interference.",
          "problem_definition": "Task: Multimodal emotion recognition from audio-text inputs under modality reliability uncertainty. Settings: (1) Faithful—text accurately describes audio emotion; (2) Adversarial—text intentionally contradicts or misleads about audio emotion; (3) Irrelevant—text is semantically unrelated to audio content. Input: Audio signal + associated text transcript/description. Output: Emotion classification robust to text interference across all three settings. Core Challenge: Detect and mitigate text-dominant bias by dynamically assessing modality credibility and routing/fusing representations accordingly, inspired by latent space intervention and modality alignment techniques from hallucination mitigation research.",
          "constraints": [
            "Robustness to adversarial and irrelevant text inputs",
            "Lightweight adaptation via small gating/adapter modules (no full model retraining)",
            "Inference-time efficiency with minimal computational overhead",
            "Task-agnostic or minimally task-specific intervention design",
            "Preservation of performance on faithful text settings"
          ],
          "technical_plan": "Core framework: Modality-credibility-based routing/fusion inspired by latent space steering and multimodal alignment approaches. (1) Construct 'text interference' representations using energy-based features (measuring representation magnitude/confidence) and temporal evidence (audio-text alignment consistency over time). (2) Train lightweight gating networks and/or adapters to estimate modality credibility scores, analogous to visual/textual intervention modules in VLM hallucination mitigation. (3) At inference, implement a routing mechanism to select between Joint (audio+text) expert or Audio-only expert based on credibility estimation—similar to self-introspective decoding strategies. (4) Training objective: Learn to detect text reliability via contrastive learning on constructed faithful/adversarial/irrelevant triplets, potentially incorporating preference optimization to distinguish reliable from misleading text. (5) Optional: Apply momentum-inspired consistency mechanisms across layers to stabilize audio feature representations under text interference.",
          "expected_contributions": [
            "First systematic study of text-dominant bias in audio-text LLMs under faithful/adversarial/irrelevant experimental settings",
            "Novel modality-credibility routing framework leveraging energy-based and temporal evidence for text interference detection, bridging hallucination mitigation techniques to audio-text domain",
            "Lightweight gating/adapter mechanism enabling robust emotion recognition without full model retraining, inspired by task-agnostic intervention approaches in VLMs",
            "Comprehensive benchmark with constructed adversarial/irrelevant text variants for evaluating audio-text model robustness"
          ],
          "evaluation_plan": "Datasets: IEMOCAP, MELD, CMU-MOSEI with constructed adversarial (emotion-contradicting) and irrelevant (random/unrelated) text variants. Metrics: Accuracy, weighted/unweighted F1, robustness drop (performance gap between faithful and adversarial/irrelevant settings), modality reliance ratio. Baselines: Standard audio-text fusion models (early/late fusion), prompt-based audio-text LLMs, existing debiasing/rebalancing methods, hallucination mitigation techniques adapted to audio-text. Ablations: (1) gating vs. adapter mechanisms, (2) energy vs. temporal features vs. combined, (3) routing threshold sensitivity, (4) expert selection strategies. Robustness analysis: varying adversarial text intensity (subtle to strong contradiction), irrelevance degree (partial to complete), and cross-dataset generalization.",
          "keywords_en": [
            "multimodal emotion recognition",
            "text-dominant bias",
            "modality routing",
            "adversarial robustness",
            "audio-text fusion",
            "lightweight adaptation",
            "credibility estimation",
            "hallucination mitigation",
            "multimodal alignment",
            "latent space steering",
            "gating mechanism",
            "audio-text large language models"
          ],
          "keywords_zh": [
            "多模态情绪识别",
            "文本主导偏见",
            "模态路由",
            "对抗鲁棒性",
            "音频-文本融合",
            "轻量级适配",
            "可信度估计",
            "幻觉缓解",
            "多模态对齐",
            "潜空间引导",
            "门控机制",
            "音频-文本大语言模型"
          ],
          "assumptions": {
            "explicit": [
              "Three experimental settings are defined: faithful, adversarial, and irrelevant text",
              "Energy-based features and temporal evidence are used for text interference representation",
              "Lightweight gating/adapter approach is employed without full model retraining",
              "Routing mechanism selects between Joint and Audio-only experts at inference time",
              "Emotion recognition is the primary downstream evaluation task"
            ],
            "inferred": [
              "A pre-trained audio-text LLM or multimodal model serves as the backbone architecture",
              "Adversarial and irrelevant text samples need to be constructed or synthesized from existing datasets",
              "Training data includes labeled emotion samples with systematic text manipulation across three settings",
              "The text interference detection module can generalize across different types of textual noise",
              "Techniques from vision-language hallucination mitigation (latent steering, preference optimization) are transferable to audio-text domain",
              "Audio modality provides more reliable ground-truth emotional cues than potentially corrupted text"
            ]
          }
        },
        "query": "Keywords: multimodal emotion recognition, text-dominant bias, modality routing, adversarial robustness, audio-text fusion, lightweight adaptation, credibility estimation, hallucination mitigation, multimodal alignment, latent space steering, gating mechanism, audio-text large language models Task: Multimodal emotion recognition from audio-text inputs under modality reliability uncertainty. Settings: (1) Faithful—text accurately describes audio emotion; (2) Adversarial—text intentionally contradicts or misleads about audio emotion; (3) Irrelevant—text is semantically unrelated to audio content. Input: Audio signal + associated text transcript/description. Output: Emotion classification robust to text interference across all three settings. Core Challenge: Detect and mitigate text-dominant bias by dynamically assessing modality credibility and routing/fusing representations accordingly, inspired by latent space intervention and modality alignment techniques from hallucination mitigation research. Constraints: Robustness to adversarial and irrelevant text inputs, Lightweight adaptation via small gating/adapter modules (no full model retraining), Inference-time efficiency with minimal computational overhead, Task-agnostic or minimally task-specific intervention design, Preservation of performance on faithful text settings Core framework: Modality-credibility-based routing/fusion inspired by latent space steering and multimodal alignment approaches. (1) Construct 'text interference' representations using energy-based features (measuring representation magnitude/confidence) and temporal evidence (audio-text alignment consistency over time). (2) Train lightweight gating networks and/or adapters to estimate modality credibility scores, analogous to visual/textual intervention modules in VLM hallucination mitigation. (3) At inference, implement a routing mechanism to select between Joint (audio+text) expert or Audio-only expert based on credibility estimation—similar to self-introspective decoding strategies. (4) Training objective: Learn to detect text reliability via contrastive learning on constructed faithful/adversarial/irrelevant triplets, potentially incorporating preference optimization to distinguish reliable from misleading text. (5) Optional: Apply momentum-inspired consistency mechanisms across layers to stabilize audio feature representations under text interference. 关键词: 多模态情绪识别，文本主导偏见，模态路由，对抗鲁棒性，音频-文本融合，轻量级适配，可信度估计，幻觉缓解，多模态对齐，潜空间引导，门控机制，音频-文本大语言模型"
      },
      {
        "pattern_id": "pattern_36",
        "pattern_name": "Theoretical Foundations of Contrastive Learning",
        "score": 0.3339391095560489,
        "brief": {
          "motivation": "Current audio-text multimodal large models exhibit text-dominant bias, where the model over-relies on textual information and neglects acoustic cues. This leads to degraded performance when text is misleading, adversarial, or irrelevant to the actual audio content—a critical vulnerability for robust real-world emotion recognition systems. Drawing from theoretical foundations of contrastive learning in multimodal settings, recent work reveals that representation alignment between modalities follows stage-wise dynamics and that contrastive pairs play a critical role in balancing learned representations. This theoretical insight motivates our investigation into how text-dominant bias emerges during multimodal fusion and how modality-credibility-based routing can restore balanced, robust representations under adversarial conditions.",
          "problem_definition": "Task: Multimodal emotion recognition from audio-text inputs with robustness to text interference. Settings: (1) Faithful—text accurately describes audio emotion; (2) Adversarial—text intentionally contradicts or misleads about audio emotion; (3) Irrelevant—text is semantically unrelated to audio content. Input: Audio signal + associated text. Output: Emotion classification that remains robust across all three settings. The core challenge is to detect when textual modality is unreliable and dynamically adjust fusion strategy, informed by theoretical understanding of how contrastive learning aligns and balances multimodal representations.",
          "constraints": [
            "Robustness to adversarial and irrelevant text interference",
            "Lightweight adaptation using small gating/adapter modules",
            "Inference-time efficiency without full model forward passes for both experts",
            "Minimal additional training overhead on top of pre-trained backbone",
            "Preservation of performance on faithful text-audio pairs"
          ],
          "technical_plan": "Core framework: Modality-credibility-based routing/fusion informed by contrastive learning dynamics. (1) Construct 'text interference' representations using energy-based features (measuring representation confidence) and temporal evidence from audio-text alignment discrepancies. (2) Leverage insights from multimodal contrastive learning theory—specifically the stage-wise alignment dynamics and rank differential mechanisms—to design credibility estimation that captures modality imbalance. (3) Train lightweight gating networks and/or adapters using contrastive objectives that distinguish faithful from adversarial/irrelevant text-audio pairs, learning to estimate modality credibility scores. (4) At inference, route to either Joint (audio+text) expert or Audio-only expert based on credibility estimation threshold. Training objective: Contrastive loss on constructed faithful/adversarial/irrelevant triplets to learn text reliability detection, combined with task-specific emotion classification loss. The contrastive formulation draws from theoretical foundations showing how contrastive pairs enable efficient representation balancing.",
          "expected_contributions": [
            "First systematic study of text-dominant bias in audio-text LLMs under faithful/adversarial/irrelevant settings with theoretical grounding from contrastive learning dynamics",
            "Novel modality-credibility routing framework leveraging energy-based and temporal evidence for text interference detection, informed by multimodal representation alignment theory",
            "Lightweight gating/adapter mechanism with contrastive training objective enabling robust emotion recognition without full model retraining",
            "Theoretical analysis connecting text-dominant bias to modality imbalance phenomena studied in contrastive learning literature"
          ],
          "evaluation_plan": "Datasets: IEMOCAP, MELD, CMU-MOSEI with constructed adversarial/irrelevant text variants (via LLM-based text manipulation and random text substitution). Metrics: Accuracy, weighted F1, robustness drop (performance gap between faithful and adversarial/irrelevant settings), routing accuracy (correct expert selection rate). Baselines: Standard audio-text fusion models (early/late fusion), prompt-based multimodal LLMs (e.g., Qwen-Audio, SALMONN), existing debiasing methods, contrastive multimodal pre-training approaches. Ablations: (a) gating vs. adapter architecture, (b) energy vs. temporal features vs. combined, (c) routing threshold sensitivity, (d) contrastive vs. non-contrastive training for credibility estimation, (e) effect of adversarial text intensity and irrelevance degree. Analysis: Visualization of learned credibility scores, representation space analysis under different text conditions, comparison with theoretical predictions from contrastive learning dynamics.",
          "keywords_en": [
            "multimodal emotion recognition",
            "text-dominant bias",
            "modality routing",
            "adversarial robustness",
            "audio-text fusion",
            "lightweight adaptation",
            "credibility estimation",
            "contrastive learning",
            "multimodal representation alignment",
            "gating mechanism",
            "energy-based detection",
            "robust multimodal learning"
          ],
          "keywords_zh": [
            "多模态情绪识别",
            "文本主导偏见",
            "模态路由",
            "对抗鲁棒性",
            "音频-文本融合",
            "轻量级适配",
            "可信度估计",
            "对比学习",
            "多模态表示对齐",
            "门控机制",
            "基于能量的检测",
            "鲁棒多模态学习"
          ],
          "assumptions": {
            "explicit": [
              "Three experimental settings are defined: faithful, adversarial, and irrelevant text",
              "Energy-based features and temporal evidence are used for text interference representation",
              "Lightweight gating/adapter approach is employed for efficiency",
              "Routing mechanism selects between Joint and Audio-only experts at inference time",
              "Contrastive training is used to learn modality credibility estimation"
            ],
            "inferred": [
              "A pre-trained audio-text multimodal LLM serves as the backbone model",
              "Adversarial and irrelevant text samples need to be constructed or synthesized (e.g., via LLM paraphrasing or random substitution)",
              "Emotion recognition is the primary downstream task for evaluation",
              "Training data includes labeled emotion samples with systematic text manipulation across three settings",
              "The text-dominant bias phenomenon is analogous to modality imbalance studied in contrastive learning theory",
              "Audio-only expert can be derived from the same backbone by masking text input or using audio-specific adapter"
            ]
          }
        },
        "query": "Keywords: multimodal emotion recognition, text-dominant bias, modality routing, adversarial robustness, audio-text fusion, lightweight adaptation, credibility estimation, contrastive learning, multimodal representation alignment, gating mechanism, energy-based detection, robust multimodal learning Task: Multimodal emotion recognition from audio-text inputs with robustness to text interference. Settings: (1) Faithful—text accurately describes audio emotion; (2) Adversarial—text intentionally contradicts or misleads about audio emotion; (3) Irrelevant—text is semantically unrelated to audio content. Input: Audio signal + associated text. Output: Emotion classification that remains robust across all three settings. The core challenge is to detect when textual modality is unreliable and dynamically adjust fusion strategy, informed by theoretical understanding of how contrastive learning aligns and balances multimodal representations. Constraints: Robustness to adversarial and irrelevant text interference, Lightweight adaptation using small gating/adapter modules, Inference-time efficiency without full model forward passes for both experts, Minimal additional training overhead on top of pre-trained backbone, Preservation of performance on faithful text-audio pairs Core framework: Modality-credibility-based routing/fusion informed by contrastive learning dynamics. (1) Construct 'text interference' representations using energy-based features (measuring representation confidence) and temporal evidence from audio-text alignment discrepancies. (2) Leverage insights from multimodal contrastive learning theory—specifically the stage-wise alignment dynamics and rank differential mechanisms—to design credibility estimation that captures modality imbalance. (3) Train lightweight gating networks and/or adapters using contrastive objectives that distinguish faithful from adversarial/irrelevant text-audio pairs, learning to estimate modality credibility scores. (4) At inference, route to either Joint (audio+text) expert or Audio-only expert based on credibility estimation threshold. Training objective: Contrastive loss on constructed faithful/adversarial/irrelevant triplets to learn text reliability detection, combined with task-specific emotion classification loss. The contrastive formulation draws from theoretical foundations showing how contrastive pairs enable efficient representation balancing. 关键词: 多模态情绪识别，文本主导偏见，模态路由，对抗鲁棒性，音频-文本融合，轻量级适配，可信度估计，对比学习，多模态表示对齐，门控机制，基于能量的检测，鲁棒多模态学习"
      },
      {
        "pattern_id": "pattern_51",
        "pattern_name": "Adversarial Vulnerabilities and Robustness in Large Language Models",
        "score": 0.32811918273557245,
        "brief": {
          "motivation": "Current audio-text multimodal large models exhibit text-dominant bias, where the model over-relies on textual information and neglects acoustic cues. This vulnerability mirrors broader adversarial robustness challenges in LLMs, where models are susceptible to subtle input manipulations—whether through chain-of-thought backdoors, jailbreak prompts, or cipher-based bypasses. In multimodal emotion recognition, adversarial or irrelevant text can mislead models similarly to how BadChain exploits reasoning pathways or how fine-tuning degrades safety alignment. This leads to degraded performance when text is misleading, adversarial, or irrelevant to the actual audio content—a critical vulnerability for robust real-world emotion recognition systems that parallels the security challenges identified in LLM robustness research.",
          "problem_definition": "Task: Multimodal emotion recognition from audio-text inputs under adversarial conditions. Settings: (1) Faithful—text accurately describes audio emotion; (2) Adversarial—text intentionally contradicts or misleads about audio emotion (analogous to backdoor/jailbreak attacks on LLMs); (3) Irrelevant—text is semantically unrelated to audio content (akin to non-natural language input vulnerabilities). Input: Audio signal + associated text. Output: Emotion classification robust to text interference across all three settings, addressing the fundamental challenge of maintaining model integrity against input-level manipulations that exploit modality-specific biases.",
          "constraints": [
            "Robustness to adversarial/irrelevant text without catastrophic performance degradation",
            "Lightweight adaptation using small adapter/gating modules (no full model retraining)",
            "Inference-time efficiency for real-world deployment",
            "Minimal additional training overhead",
            "Preservation of performance on faithful text-audio pairs",
            "Generalization across different adversarial text generation strategies"
          ],
          "technical_plan": "Core framework: Modality-credibility-based routing/fusion inspired by adversarial robustness principles. (1) Construct 'text interference' representations using energy-based features (detecting anomalous text patterns similar to cipher/non-natural language detection) and temporal evidence from audio-text alignment (identifying mismatches analogous to chain-of-thought inconsistencies). (2) Train lightweight gating networks and/or adapters to estimate modality credibility scores, learning to detect unreliable text inputs similar to how robustness frameworks identify adversarial prompts. (3) At inference, route to either Joint (audio+text) expert or Audio-only expert based on credibility estimation, implementing a dynamic defense mechanism. (4) Training objective: Learn to detect text reliability via contrastive learning on faithful/adversarial/irrelevant triplets, potentially incorporating adversarial training strategies from LLM robustness literature. (5) Consider hierarchical detection inspired by AutoDAN's genetic approach for identifying subtle adversarial patterns.",
          "expected_contributions": [
            "First systematic study of text-dominant bias in audio-text LLMs under faithful/adversarial/irrelevant settings, connecting multimodal robustness to broader LLM security challenges",
            "Novel modality-credibility routing framework with energy and temporal evidence for text interference detection, analogous to adversarial input detection in LLMs",
            "Lightweight gating/adapter mechanism enabling robust emotion recognition without full model retraining, providing efficient defense against modality-level attacks",
            "Comprehensive adversarial text construction methodology for multimodal settings, extending jailbreak/backdoor concepts to audio-text domains",
            "Empirical analysis of how text manipulation strategies affect multimodal model behavior, contributing to understanding of cross-modal adversarial vulnerabilities"
          ],
          "evaluation_plan": "Datasets: IEMOCAP, MELD, CMU-MOSEI with constructed adversarial/irrelevant text variants using multiple generation strategies (rule-based, LLM-generated, gradient-based). Metrics: Accuracy, weighted/macro F1, robustness drop (performance gap between faithful and adversarial/irrelevant settings), attack success rate (ASR) for adversarial text. Baselines: Standard audio-text fusion models (early/late fusion), prompt-based multimodal LLMs, existing debiasing methods, adversarial training baselines from LLM robustness literature. Ablations: gating vs. adapter architecture, energy vs. temporal features, routing threshold sensitivity, credibility estimation methods. Robustness settings: varying adversarial text intensity (subtle to overt contradictions), irrelevance degree (topically related to completely random), and adversarial generation methods (inspired by AutoDAN, BadChain attack patterns). Additional analysis: transferability of adversarial text across models, detection-evasion trade-offs.",
          "keywords_en": [
            "multimodal emotion recognition",
            "text-dominant bias",
            "modality routing",
            "adversarial robustness",
            "audio-text fusion",
            "lightweight adaptation",
            "credibility estimation",
            "multimodal adversarial attacks",
            "large language model security",
            "cross-modal robustness",
            "gating mechanism",
            "modality-aware defense"
          ],
          "keywords_zh": [
            "多模态情绪识别",
            "文本主导偏见",
            "模态路由",
            "对抗鲁棒性",
            "音频-文本融合",
            "轻量级适配",
            "可信度估计",
            "多模态对抗攻击",
            "大语言模型安全",
            "跨模态鲁棒性"
          ],
          "assumptions": {
            "explicit": [
              "Three experimental settings: faithful, adversarial, irrelevant text",
              "Use energy and temporal evidence for text interference representation",
              "Lightweight gating/adapter approach without full model retraining",
              "Routing between Joint and Audio-only experts at inference time",
              "Focus on emotion recognition as the downstream task"
            ],
            "inferred": [
              "Pre-trained audio-text LLM or multimodal model serves as backbone",
              "Adversarial/irrelevant text samples need to be constructed or synthesized (potentially using LLM-based generation)",
              "Training data includes labeled emotion samples with systematic text manipulation",
              "Audio modality provides ground-truth emotional signal when text is unreliable",
              "Text interference patterns share characteristics with adversarial prompts in LLM security literature",
              "Energy-based detection may leverage distributional differences between natural and adversarial text",
              "Temporal evidence assumes audio-text alignment information is available or computable"
            ]
          }
        },
        "query": "Keywords: multimodal emotion recognition, text-dominant bias, modality routing, adversarial robustness, audio-text fusion, lightweight adaptation, credibility estimation, multimodal adversarial attacks, large language model security, cross-modal robustness, gating mechanism, modality-aware defense Task: Multimodal emotion recognition from audio-text inputs under adversarial conditions. Settings: (1) Faithful—text accurately describes audio emotion; (2) Adversarial—text intentionally contradicts or misleads about audio emotion (analogous to backdoor/jailbreak attacks on LLMs); (3) Irrelevant—text is semantically unrelated to audio content (akin to non-natural language input vulnerabilities). Input: Audio signal + associated text. Output: Emotion classification robust to text interference across all three settings, addressing the fundamental challenge of maintaining model integrity against input-level manipulations that exploit modality-specific biases. Constraints: Robustness to adversarial/irrelevant text without catastrophic performance degradation, Lightweight adaptation using small adapter/gating modules (no full model retraining), Inference-time efficiency for real-world deployment, Minimal additional training overhead, Preservation of performance on faithful text-audio pairs, Generalization across different adversarial text generation strategies Core framework: Modality-credibility-based routing/fusion inspired by adversarial robustness principles. (1) Construct 'text interference' representations using energy-based features (detecting anomalous text patterns similar to cipher/non-natural language detection) and temporal evidence from audio-text alignment (identifying mismatches analogous to chain-of-thought inconsistencies). (2) Train lightweight gating networks and/or adapters to estimate modality credibility scores, learning to detect unreliable text inputs similar to how robustness frameworks identify adversarial prompts. (3) At inference, route to either Joint (audio+text) expert or Audio-only expert based on credibility estimation, implementing a dynamic defense mechanism. (4) Training objective: Learn to detect text reliability via contrastive learning on faithful/adversarial/irrelevant triplets, potentially incorporating adversarial training strategies from LLM robustness literature. (5) Consider hierarchical detection inspired by AutoDAN's genetic approach for identifying subtle adversarial patterns. 关键词: 多模态情绪识别，文本主导偏见，模态路由，对抗鲁棒性，音频-文本融合，轻量级适配，可信度估计，多模态对抗攻击，大语言模型安全，跨模态鲁棒性"
      }
    ],
    "judge": {
      "best_index": 0,
      "rationale": "All three candidates faithfully capture the core idea of addressing text-dominant bias in audio-text models via modality-credibility routing with energy/temporal evidence and lightweight gating. However, Pattern 13 (Hallucination Mitigation via Multimodal Alignment) provides the most coherent and actionable framing. Its motivation draws a clear analogy to the well-studied VLM hallucination problem, making the research context immediately accessible. The technical plan is most complete, explicitly mentioning momentum-inspired consistency mechanisms and preference optimization as optional enhancements. The evaluation plan includes the crucial 'modality reliance ratio' metric specifically designed to measure text-dominant bias, and provides the most detailed ablation structure. Pattern 36 forces a somewhat awkward connection to contrastive learning theory that doesn't naturally fit the routing/gating approach (the idea uses contrastive learning for training but the 'theoretical foundations' angle adds complexity without clear benefit). Pattern 51's framing around LLM adversarial vulnerabilities (jailbreaks, backdoors) is less appropriate since the core problem is modality imbalance rather than security attacks—the analogy feels stretched and could mislead readers. Pattern 13's assumptions are also most precisely stated, explicitly noting the transferability hypothesis from VLM techniques to audio-text domain."
    },
    "recall_scores": {
      "0": 0.04,
      "1": 0.04,
      "2": 0.02857142857142857
    },
    "chosen_index": 0,
    "query_best": "Keywords: multimodal emotion recognition, text-dominant bias, modality routing, adversarial robustness, audio-text fusion, lightweight adaptation, credibility estimation, hallucination mitigation, multimodal alignment, latent space steering, gating mechanism, audio-text large language models Task: Multimodal emotion recognition from audio-text inputs under modality reliability uncertainty. Settings: (1) Faithful—text accurately describes audio emotion; (2) Adversarial—text intentionally contradicts or misleads about audio emotion; (3) Irrelevant—text is semantically unrelated to audio content. Input: Audio signal + associated text transcript/description. Output: Emotion classification robust to text interference across all three settings. Core Challenge: Detect and mitigate text-dominant bias by dynamically assessing modality credibility and routing/fusing representations accordingly, inspired by latent space intervention and modality alignment techniques from hallucination mitigation research. Constraints: Robustness to adversarial and irrelevant text inputs, Lightweight adaptation via small gating/adapter modules (no full model retraining), Inference-time efficiency with minimal computational overhead, Task-agnostic or minimally task-specific intervention design, Preservation of performance on faithful text settings Core framework: Modality-credibility-based routing/fusion inspired by latent space steering and multimodal alignment approaches. (1) Construct 'text interference' representations using energy-based features (measuring representation magnitude/confidence) and temporal evidence (audio-text alignment consistency over time). (2) Train lightweight gating networks and/or adapters to estimate modality credibility scores, analogous to visual/textual intervention modules in VLM hallucination mitigation. (3) At inference, implement a routing mechanism to select between Joint (audio+text) expert or Audio-only expert based on credibility estimation—similar to self-introspective decoding strategies. (4) Training objective: Learn to detect text reliability via contrastive learning on constructed faithful/adversarial/irrelevant triplets, potentially incorporating preference optimization to distinguish reliable from misleading text. (5) Optional: Apply momentum-inspired consistency mechanisms across layers to stabilize audio feature representations under text interference. 关键词: 多模态情绪识别，文本主导偏见，模态路由，对抗鲁棒性，音频-文本融合，轻量级适配，可信度估计，幻觉缓解，多模态对齐，潜空间引导，门控机制，音频-文本大语言模型",
    "brief_best": {
      "motivation": "Current audio-text multimodal large models exhibit text-dominant bias, where the model over-relies on textual information and neglects acoustic cues—analogous to the hallucination problem in vision-language models where misalignment between modalities leads to unreliable outputs. This text-over-reliance leads to degraded performance when text is misleading, adversarial, or irrelevant to the actual audio content. Drawing from recent advances in hallucination mitigation via multimodal alignment and latent space steering in VLMs, addressing this modality imbalance is critical for building robust real-world emotion recognition systems that can reliably ground predictions in acoustic evidence even under textual interference.",
      "problem_definition": "Task: Multimodal emotion recognition from audio-text inputs under modality reliability uncertainty. Settings: (1) Faithful—text accurately describes audio emotion; (2) Adversarial—text intentionally contradicts or misleads about audio emotion; (3) Irrelevant—text is semantically unrelated to audio content. Input: Audio signal + associated text transcript/description. Output: Emotion classification robust to text interference across all three settings. Core Challenge: Detect and mitigate text-dominant bias by dynamically assessing modality credibility and routing/fusing representations accordingly, inspired by latent space intervention and modality alignment techniques from hallucination mitigation research.",
      "constraints": [
        "Robustness to adversarial and irrelevant text inputs",
        "Lightweight adaptation via small gating/adapter modules (no full model retraining)",
        "Inference-time efficiency with minimal computational overhead",
        "Task-agnostic or minimally task-specific intervention design",
        "Preservation of performance on faithful text settings"
      ],
      "technical_plan": "Core framework: Modality-credibility-based routing/fusion inspired by latent space steering and multimodal alignment approaches. (1) Construct 'text interference' representations using energy-based features (measuring representation magnitude/confidence) and temporal evidence (audio-text alignment consistency over time). (2) Train lightweight gating networks and/or adapters to estimate modality credibility scores, analogous to visual/textual intervention modules in VLM hallucination mitigation. (3) At inference, implement a routing mechanism to select between Joint (audio+text) expert or Audio-only expert based on credibility estimation—similar to self-introspective decoding strategies. (4) Training objective: Learn to detect text reliability via contrastive learning on constructed faithful/adversarial/irrelevant triplets, potentially incorporating preference optimization to distinguish reliable from misleading text. (5) Optional: Apply momentum-inspired consistency mechanisms across layers to stabilize audio feature representations under text interference.",
      "expected_contributions": [
        "First systematic study of text-dominant bias in audio-text LLMs under faithful/adversarial/irrelevant experimental settings",
        "Novel modality-credibility routing framework leveraging energy-based and temporal evidence for text interference detection, bridging hallucination mitigation techniques to audio-text domain",
        "Lightweight gating/adapter mechanism enabling robust emotion recognition without full model retraining, inspired by task-agnostic intervention approaches in VLMs",
        "Comprehensive benchmark with constructed adversarial/irrelevant text variants for evaluating audio-text model robustness"
      ],
      "evaluation_plan": "Datasets: IEMOCAP, MELD, CMU-MOSEI with constructed adversarial (emotion-contradicting) and irrelevant (random/unrelated) text variants. Metrics: Accuracy, weighted/unweighted F1, robustness drop (performance gap between faithful and adversarial/irrelevant settings), modality reliance ratio. Baselines: Standard audio-text fusion models (early/late fusion), prompt-based audio-text LLMs, existing debiasing/rebalancing methods, hallucination mitigation techniques adapted to audio-text. Ablations: (1) gating vs. adapter mechanisms, (2) energy vs. temporal features vs. combined, (3) routing threshold sensitivity, (4) expert selection strategies. Robustness analysis: varying adversarial text intensity (subtle to strong contradiction), irrelevance degree (partial to complete), and cross-dataset generalization.",
      "keywords_en": [
        "multimodal emotion recognition",
        "text-dominant bias",
        "modality routing",
        "adversarial robustness",
        "audio-text fusion",
        "lightweight adaptation",
        "credibility estimation",
        "hallucination mitigation",
        "multimodal alignment",
        "latent space steering",
        "gating mechanism",
        "audio-text large language models"
      ],
      "keywords_zh": [
        "多模态情绪识别",
        "文本主导偏见",
        "模态路由",
        "对抗鲁棒性",
        "音频-文本融合",
        "轻量级适配",
        "可信度估计",
        "幻觉缓解",
        "多模态对齐",
        "潜空间引导",
        "门控机制",
        "音频-文本大语言模型"
      ],
      "assumptions": {
        "explicit": [
          "Three experimental settings are defined: faithful, adversarial, and irrelevant text",
          "Energy-based features and temporal evidence are used for text interference representation",
          "Lightweight gating/adapter approach is employed without full model retraining",
          "Routing mechanism selects between Joint and Audio-only experts at inference time",
          "Emotion recognition is the primary downstream evaluation task"
        ],
        "inferred": [
          "A pre-trained audio-text LLM or multimodal model serves as the backbone architecture",
          "Adversarial and irrelevant text samples need to be constructed or synthesized from existing datasets",
          "Training data includes labeled emotion samples with systematic text manipulation across three settings",
          "The text interference detection module can generalize across different types of textual noise",
          "Techniques from vision-language hallucination mitigation (latent steering, preference optimization) are transferable to audio-text domain",
          "Audio modality provides more reliable ground-truth emotional cues than potentially corrupted text"
        ]
      }
    }
  }
}