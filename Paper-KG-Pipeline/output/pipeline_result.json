{
  "user_idea": "我们研究音频-文本大模型的文本主导偏见：在 faithful/adversarial/irrelevant 三类设定下，提出基于模态可信度的路由/融合框架。通过构造“文本干扰”表示能量与时序证据，训练轻量门控/适配器，在推理时选择 Joint 或 Audio 专家，实现对抗文本误导下的稳健情绪识别",
  "success": true,
  "iterations": 1,
  "selected_patterns": {
    "stability": [
      "pattern_74",
      "pattern_112",
      "pattern_9",
      "pattern_67",
      "pattern_113"
    ],
    "novelty": [
      "pattern_112",
      "pattern_7",
      "pattern_113",
      "pattern_67",
      "pattern_8"
    ],
    "domain_distance": [
      "pattern_113",
      "pattern_13",
      "pattern_67",
      "pattern_9",
      "pattern_7"
    ]
  },
  "final_story": {
    "title": "Overcoming Text-Dominant Bias via Modality Reliability Routing for Arbitrary Audio-Text Conditions",
    "abstract": "Audio-Text Large Models (AT-LLMs) often exhibit a 'text-dominant bias,' over-relying on textual input and leading to performance degradation when text is adversarial or irrelevant. We address this by reframing multimodal fusion as a dynamic routing task based on modality reliability. We propose a framework that constructs a 'Text Interference' representation utilizing energy and temporal evidence to quantify cross-modality discrepancies. A lightweight gating mechanism is trained to assess this reliability, dynamically routing inputs to a Joint Multimodal Expert for faithful cases or an Audio-only Expert for adversarial ones. This approach ensures robust emotion recognition under arbitrary modality conditions, significantly outperforming static fusion baselines by prioritizing reliable unimodal evidence when cross-modal alignment fails.",
    "problem_framing": "We reframe the challenge of multimodal emotion recognition from a static feature fusion problem to a dynamic management of arbitrary modality conditions. The critical issue is the text-dominant bias, where the model over-relies on text inputs due to imbalanced dependency structures. This reframing shifts the focus from maximizing cross-modal alignment to identifying and mitigating scenarios where text provides misleading evidence, such as adversarial or irrelevant settings, thereby highlighting the need for robust handling of modality discrepancies.",
    "gap_pattern": "Current approaches to Audio-Text LLMs fail to address text-dominant bias because they treat cross-modal interaction as a static, unconditional process. These methods assume modalities are generally consistent and lack the mechanism to detect when text is misleading. By reframing the fusion process as a fixed aggregation rather than a conditional decision, existing models cannot effectively handle arbitrary modality conditions, leading to performance collapse when the dominant modality (text) conflicts with the target emotion signal in audio.",
    "solution": "Our solution transforms the fusion paradigm by introducing a Modality Reliability Routing framework that actively detects and responds to text-dominant bias. We utilize energy and temporal evidence to construct a 'Text Interference' representation, quantifying the reliability of the text modality relative to audio. This representation drives a lightweight gating mechanism that learns the dependency structure between modalities. During inference, the system dynamically routes the input to either a Joint Multimodal Expert (for faithful, consistent cases) or an Audio-only Expert (for adversarial/irrelevant cases). This approach effectively mitigates bias by leveraging strong unimodal evidence when cross-modal trust breaks down.",
    "method_skeleton": "Re-analyze the inference pipeline of Audio-Text LLMs to incorporate a lightweight gating mechanism that optimizes robustness under arbitrary modality conditions; Utilize a 'Text Interference' framework to model the dependency dynamics between audio and text, extending it to include energy and temporal evidence for reliability assessment; Analyze static fusion limitations and introduce a Dynamic Routing strategy to select between a Joint Multimodal Expert and an Audio-only Expert based on the learned modality trustworthiness.",
    "innovation_claims": [
      "Transform multimodal learning from static fusion to Modality Reliability Routing, effectively mitigating text-dominant bias by dynamically selecting experts based on cross-modal dependency structures.",
      "Reframe the handling of misleading text as a quantifiable 'Text Interference' problem by leveraging energy and temporal evidence, enabling precise assessment of modality trustworthiness.",
      "Shift the robustness paradigm for emotion recognition to handle arbitrary modality conditions, ensuring stability under adversarial and irrelevant settings by prioritizing reliable unimodal audio evidence."
    ],
    "experiments_plan": "We evaluate on IEMOCAP and MELD, constructing adversarial and irrelevant splits to simulate arbitrary modality conditions. Metrics include Accuracy and Weighted F1-score. We compare against standard Audio-Text LLMs and static fusion baselines. Ablation studies will isolate the impact of the interference representation, the gating mechanism, and the routing strategy on robustness."
  },
  "review_history": [
    {
      "pass": true,
      "avg_score": 6.803333333333232,
      "reviews": [
        {
          "reviewer": "Reviewer A",
          "role": "Methodology",
          "score": 5.169999999999933,
          "feedback": "Blind comparisons vs 11 anchors. Loss=9.2266, AvgStrength=2.00. CoachPriority: method_skeleton, innovation_claims, abstract."
        },
        {
          "reviewer": "Reviewer B",
          "role": "Novelty",
          "score": 6.949999999999895,
          "feedback": "Blind comparisons vs 11 anchors. Loss=12.9497, AvgStrength=2.00. CoachPriority: method_skeleton, innovation_claims, abstract."
        },
        {
          "reviewer": "Reviewer C",
          "role": "Storyteller",
          "score": 8.289999999999868,
          "feedback": "Blind comparisons vs 11 anchors. Loss=4.8269, AvgStrength=1.64. CoachPriority: method_skeleton, innovation_claims, abstract."
        }
      ],
      "main_issue": "stability",
      "suggestions": [
        "从stability维度选择稳健Pattern",
        "注入成熟方法增强鲁棒性"
      ],
      "audit": {
        "pattern_id": "pattern_74",
        "anchors": [
          {
            "anchor_id": "A1",
            "paper_id": "79ZkWgY2FI",
            "score10": 5.787999999999999,
            "weight": 0.6399140961528769
          },
          {
            "anchor_id": "A2",
            "paper_id": "4es2oO9tw1",
            "score10": 5.967999999999999,
            "weight": 0.7928139244371923
          },
          {
            "anchor_id": "A3",
            "paper_id": "i0zzO7Hslk",
            "score10": 6.004,
            "weight": 1.0417206216442176
          },
          {
            "anchor_id": "A4",
            "paper_id": "gU4ZgQNsOC",
            "score10": 6.039999999999999,
            "weight": 0.9357197165314532
          },
          {
            "anchor_id": "A5",
            "paper_id": "BTKAeLqLMw",
            "score10": 6.175,
            "weight": 0.7121406692186284
          },
          {
            "anchor_id": "A6",
            "paper_id": "OQqNieeivq",
            "score10": 6.22,
            "weight": 0.8610221898474837
          },
          {
            "anchor_id": "A7",
            "paper_id": "3E8YNv1HjU",
            "score10": 6.292000000000001,
            "weight": 0.7928139244371923
          },
          {
            "anchor_id": "A8",
            "paper_id": "csbf1p8xUq",
            "score10": 6.49,
            "weight": 0.8470725854916313
          },
          {
            "anchor_id": "A9",
            "paper_id": "wg1PCg3CUP",
            "score10": 6.724,
            "weight": 1.5184402281593683
          },
          {
            "anchor_id": "A10",
            "paper_id": "-Aw0rrrPUF",
            "score10": 7.624,
            "weight": 0.6503664135129055
          },
          {
            "anchor_id": "A11",
            "paper_id": "gUL6zYN4Uaf",
            "score10": 5.788,
            "weight": 0.5752036819351701
          }
        ],
        "anchors_rounds": [
          [
            {
              "anchor_id": "A1",
              "paper_id": "79ZkWgY2FI",
              "score10": 5.787999999999999,
              "weight": 0.6399140961528769
            },
            {
              "anchor_id": "A2",
              "paper_id": "4es2oO9tw1",
              "score10": 5.967999999999999,
              "weight": 0.7928139244371923
            },
            {
              "anchor_id": "A3",
              "paper_id": "i0zzO7Hslk",
              "score10": 6.004,
              "weight": 1.0417206216442176
            },
            {
              "anchor_id": "A4",
              "paper_id": "gU4ZgQNsOC",
              "score10": 6.039999999999999,
              "weight": 0.9357197165314532
            },
            {
              "anchor_id": "A5",
              "paper_id": "BTKAeLqLMw",
              "score10": 6.175,
              "weight": 0.7121406692186284
            },
            {
              "anchor_id": "A6",
              "paper_id": "OQqNieeivq",
              "score10": 6.22,
              "weight": 0.8610221898474837
            },
            {
              "anchor_id": "A7",
              "paper_id": "3E8YNv1HjU",
              "score10": 6.292000000000001,
              "weight": 0.7928139244371923
            },
            {
              "anchor_id": "A8",
              "paper_id": "csbf1p8xUq",
              "score10": 6.49,
              "weight": 0.8470725854916313
            },
            {
              "anchor_id": "A9",
              "paper_id": "wg1PCg3CUP",
              "score10": 6.724,
              "weight": 1.5184402281593683
            },
            {
              "anchor_id": "A10",
              "paper_id": "-Aw0rrrPUF",
              "score10": 7.624,
              "weight": 0.6503664135129055
            },
            {
              "anchor_id": "A11",
              "paper_id": "gUL6zYN4Uaf",
              "score10": 5.788,
              "weight": 0.5752036819351701
            }
          ]
        ],
        "role_details": {
          "Methodology": {
            "comparisons": [
              {
                "anchor_id": "A1",
                "judgement": "better",
                "strength": "medium",
                "rationale": "Story presents more specific technical solution with lightweight gating mechanism and Text Interference framework."
              },
              {
                "anchor_id": "A2",
                "judgement": "better",
                "strength": "medium",
                "rationale": "Story's lightweight gating mechanism and Text Interference framework are more specific and innovative than A2's cost-aware utility function."
              },
              {
                "anchor_id": "A3",
                "judgement": "worse",
                "strength": "medium",
                "rationale": "A3's Low-rank Riemannian Optimizer is more mathematically rigorous and generalizable than Story's specific solution."
              },
              {
                "anchor_id": "A4",
                "judgement": "better",
                "strength": "medium",
                "rationale": "Story's lightweight gating mechanism and Text Interference framework are more innovative than A4's dynamic data reweighting."
              },
              {
                "anchor_id": "A5",
                "judgement": "worse",
                "strength": "medium",
                "rationale": "A5's controlled studies and measurement techniques suggest more rigorous experimental approach than Story's specific solution."
              },
              {
                "anchor_id": "A6",
                "judgement": "worse",
                "strength": "medium",
                "rationale": "A6's SVD with knowledge-aware singular values is more mathematically rigorous than Story's specific solution."
              },
              {
                "anchor_id": "A7",
                "judgement": "worse",
                "strength": "medium",
                "rationale": "A7's taxonomy and predictive model suggest more systematic approach than Story's specific solution."
              },
              {
                "anchor_id": "A8",
                "judgement": "worse",
                "strength": "medium",
                "rationale": "A8's modular architecture and adaptive optimization are more comprehensive than Story's specific solution."
              },
              {
                "anchor_id": "A9",
                "judgement": "worse",
                "strength": "medium",
                "rationale": "A9's precision-aware scaling laws are more theoretically grounded than Story's specific solution."
              },
              {
                "anchor_id": "A10",
                "judgement": "worse",
                "strength": "medium",
                "rationale": "A10's large-scale engineering effort to develop GLM-130B is more significant than Story's specific solution."
              },
              {
                "anchor_id": "A11",
                "judgement": "worse",
                "strength": "medium",
                "rationale": "A11's focus on practical feasibility under extreme constraints is more comprehensive than Story's specific solution."
              }
            ],
            "loss": 9.226630217273293,
            "avg_strength": 2.0,
            "monotonic_violations": 2,
            "ci_low": 4.009999999999958,
            "ci_high": 6.159999999999912,
            "tau": 1.0
          },
          "Novelty": {
            "comparisons": [
              {
                "anchor_id": "A1",
                "judgement": "better",
                "strength": "medium",
                "rationale": "Story introduces novel 'Text Interference' framework and Modality Reliability Routing, offering more specific innovation than A1's data distribution analysis."
              },
              {
                "anchor_id": "A2",
                "judgement": "better",
                "strength": "medium",
                "rationale": "Story introduces novel 'Text Interference' framework and Modality Reliability Routing, offering more specific innovation than A2's cost-aware data selection."
              },
              {
                "anchor_id": "A3",
                "judgement": "tie",
                "strength": "medium",
                "rationale": "Both introduce novel frameworks in their respective domains - Story's 'Text Interference' vs. A3's Low-rank Riemannian Optimizer."
              },
              {
                "anchor_id": "A4",
                "judgement": "tie",
                "strength": "medium",
                "rationale": "Both introduce novel frameworks in their respective domains - Story's 'Text Interference' vs. A4's dynamic data reweighting."
              },
              {
                "anchor_id": "A5",
                "judgement": "tie",
                "strength": "medium",
                "rationale": "Both introduce novel frameworks in their respective domains - Story's 'Text Interference' vs. A5's measurement-driven data selection."
              },
              {
                "anchor_id": "A6",
                "judgement": "tie",
                "strength": "medium",
                "rationale": "Both introduce novel frameworks in their respective domains - Story's 'Text Interference' vs. A6's knowledge-aware adaptation using SVD."
              },
              {
                "anchor_id": "A7",
                "judgement": "tie",
                "strength": "medium",
                "rationale": "Both introduce novel frameworks in their respective domains - Story's 'Text Interference' vs. A7's taxonomy of memorization types."
              },
              {
                "anchor_id": "A8",
                "judgement": "tie",
                "strength": "medium",
                "rationale": "Both introduce novel frameworks in their respective domains - Story's 'Text Interference' vs. A8's modular architecture and optimization."
              },
              {
                "anchor_id": "A9",
                "judgement": "tie",
                "strength": "medium",
                "rationale": "Both introduce novel frameworks in their respective domains - Story's 'Text Interference' vs. A9's precision-aware scaling laws."
              },
              {
                "anchor_id": "A10",
                "judgement": "better",
                "strength": "medium",
                "rationale": "Story introduces novel 'Text Interference' framework and Modality Reliability Routing, offering more methodological innovation than A10's model development."
              },
              {
                "anchor_id": "A11",
                "judgement": "tie",
                "strength": "medium",
                "rationale": "Both introduce novel frameworks in their respective domains - Story's 'Text Interference' vs. A11's modified pretraining pipeline."
              }
            ],
            "loss": 12.949678496324307,
            "avg_strength": 2.0,
            "monotonic_violations": 2,
            "ci_low": 5.659999999999923,
            "ci_high": 8.329999999999867,
            "tau": 1.4
          },
          "Storyteller": {
            "comparisons": [
              {
                "anchor_id": "A1",
                "judgement": "better",
                "strength": "medium",
                "rationale": "Story has more specific technical approach with clearer claims about text-dominant bias."
              },
              {
                "anchor_id": "A2",
                "judgement": "better",
                "strength": "medium",
                "rationale": "Story has more specific technical approach with clearer claims about multimodal emotion recognition."
              },
              {
                "anchor_id": "A3",
                "judgement": "better",
                "strength": "medium",
                "rationale": "Story has more specific technical approach with clearer claims about dynamic modality management."
              },
              {
                "anchor_id": "A4",
                "judgement": "better",
                "strength": "medium",
                "rationale": "Story has more specific technical approach with clearer claims about cross-modal dependency structures."
              },
              {
                "anchor_id": "A5",
                "judgement": "better",
                "strength": "medium",
                "rationale": "Story has more specific technical approach with clearer claims about Text Interference framework."
              },
              {
                "anchor_id": "A6",
                "judgement": "better",
                "strength": "medium",
                "rationale": "Story has more specific technical approach with clearer claims about Modality Reliability Routing."
              },
              {
                "anchor_id": "A7",
                "judgement": "better",
                "strength": "medium",
                "rationale": "Story has more specific technical approach with clearer claims about gating mechanism."
              },
              {
                "anchor_id": "A8",
                "judgement": "tie",
                "strength": "weak",
                "rationale": "Both have specific methodological approaches and clear claims, making them comparable."
              },
              {
                "anchor_id": "A9",
                "judgement": "tie",
                "strength": "weak",
                "rationale": "Both have specific methodological approaches and clear claims, making them comparable."
              },
              {
                "anchor_id": "A10",
                "judgement": "tie",
                "strength": "weak",
                "rationale": "Both have specific methodological approaches and clear claims, making them comparable."
              },
              {
                "anchor_id": "A11",
                "judgement": "tie",
                "strength": "weak",
                "rationale": "Both have specific methodological approaches and clear claims, making them comparable."
              }
            ],
            "loss": 4.826897430951426,
            "avg_strength": 1.6363636363636365,
            "monotonic_violations": 1,
            "ci_low": 6.939999999999896,
            "ci_high": 9.999999999999831,
            "tau": 1.0
          }
        },
        "pass": {
          "mode": "two_of_three_q75_and_avg_ge_q50",
          "used_distribution": "pattern",
          "pattern_paper_count": 156,
          "q50": 6.175,
          "q75": 6.292000000000001,
          "count_roles_ge_q75": 2,
          "roles_ge_q75": {
            "Methodology": false,
            "Novelty": true,
            "Storyteller": true
          },
          "avg_ge_q50": true,
          "avg_score": 6.803333333333232
        },
        "rubric_version": "rubric_v1",
        "card_version": "blind_card_v2_minimal"
      },
      "field_feedback": {
        "title": {
          "issue": "The phrase 'Arbitrary Audio-Text Conditions' is vague and does not clearly convey the core challenge of stability under adversarial or noisy inputs.",
          "edit_instruction": "Replace 'Arbitrary Audio-Text Conditions' with 'Noisy and Adversarial Modality Conditions' to be precise about the stability challenges addressed.",
          "expected_effect": "Immediately clarifies the scope of the problem and the specific type of instability the paper tackles."
        },
        "abstract": {
          "issue": "The description of the 'Text Interference' representation is too high-level; it lacks technical specificity on how 'energy and temporal evidence' are mathematically modeled to ensure stability.",
          "edit_instruction": "Explicitly mention the mechanism for calculating interference (e.g., 'computing cross-modal attention entropy' or 'spectral-temporal divergence') to ground the stability claim in a concrete method.",
          "expected_effect": "Provides technical depth that reassures reviewers about the feasibility of the stability solution."
        },
        "problem_framing": {
          "issue": "It frames the issue primarily as 'bias' rather than 'instability.' The connection between static fusion and unstable performance under distribution shift is weak.",
          "edit_instruction": "Reframe the problem to emphasize that static fusion weights create a fragile system that collapses when modality reliability shifts, leading to unstable inference.",
          "expected_effect": "Aligns the problem statement directly with the main issue (stability) and justifies the need for a dynamic solution."
        },
        "method_skeleton": {
          "issue": "The language is passive and process-oriented ('Re-analyze,' 'Utilize') rather than algorithmic. It fails to describe *how* the gate learns stability or the specific training objective.",
          "edit_instruction": "Rewrite to describe the algorithmic flow: 'We formulate a reliability loss function that minimizes audio-text discrepancy variance. The gating mechanism is trained via reinforcement learning to maximize reward on reliable joint features.'",
          "expected_effect": "Transforms a vague description into a concrete technical contribution, addressing the low Methodology score."
        },
        "innovation_claims": {
          "issue": "The term 'Text Interference' sounds metaphorical rather than a novel technical contribution. The claims overlap significantly between 'routing' and 'handling misleading text.'",
          "edit_instruction": "Differentiate the claims: 1) A novel 'Modulation Instability Index' (quantifying interference), 2) A 'Reliability-Gated Fusion' architecture, and 3) Demonstration of robustness on zero-shot adversarial splits.",
          "expected_effect": "Clarifies the distinct contributions (metric + architecture + evaluation) and sharpens the novelty."
        },
        "experiments_plan": {
          "issue": "The definition of 'arbitrary modality conditions' is operationally vague. It lacks a specific metric to measure 'stability' beyond standard Accuracy/F1.",
          "edit_instruction": "Define the construction of adversarial splits (e.g., 'mismatched transcripts' or 'low SNR audio'). Add a 'Stability Score' metric (e.g., standard deviation of performance across noise levels) to quantify robustness.",
          "expected_effect": "Makes the experimental setup reproducible and directly measures the paper's main claim (stability)."
        }
      },
      "suggested_edits": [
        {
          "field": "innovation_claims",
          "action": "rewrite",
          "content": "1. Introduce a 'Modulation Instability Index' (MII) that quantifies text interference using energy-temporal divergence, providing a mathematical basis for modality trustworthiness.\n2. Propose a 'Reliability-Gated Fusion' architecture that dynamically switches between joint and unimodal experts to minimize inference variance under noisy conditions.\n3. Establish a new benchmark for 'Stable Emotion Recognition' by evaluating performance degradation across systematically constructed adversarial audio-text mismatches."
        },
        {
          "field": "method_skeleton",
          "action": "rewrite",
          "content": "Formulate a reliability-aware inference pipeline where a Gating Network predicts a confidence score based on the alignment between audio spectral features and text embeddings. Construct a 'Text Interference' loss function that penalizes high cross-modal discrepancy during training. Implement a Dynamic Routing strategy that directs the input to a Joint Multimodal Expert when confidence > threshold, or an Audio-only Expert otherwise, ensuring stable feature integration."
        }
      ],
      "priority": [
        "method_skeleton",
        "innovation_claims",
        "abstract"
      ],
      "review_coach": {
        "field_feedback": {
          "title": {
            "issue": "The phrase 'Arbitrary Audio-Text Conditions' is vague and does not clearly convey the core challenge of stability under adversarial or noisy inputs.",
            "edit_instruction": "Replace 'Arbitrary Audio-Text Conditions' with 'Noisy and Adversarial Modality Conditions' to be precise about the stability challenges addressed.",
            "expected_effect": "Immediately clarifies the scope of the problem and the specific type of instability the paper tackles."
          },
          "abstract": {
            "issue": "The description of the 'Text Interference' representation is too high-level; it lacks technical specificity on how 'energy and temporal evidence' are mathematically modeled to ensure stability.",
            "edit_instruction": "Explicitly mention the mechanism for calculating interference (e.g., 'computing cross-modal attention entropy' or 'spectral-temporal divergence') to ground the stability claim in a concrete method.",
            "expected_effect": "Provides technical depth that reassures reviewers about the feasibility of the stability solution."
          },
          "problem_framing": {
            "issue": "It frames the issue primarily as 'bias' rather than 'instability.' The connection between static fusion and unstable performance under distribution shift is weak.",
            "edit_instruction": "Reframe the problem to emphasize that static fusion weights create a fragile system that collapses when modality reliability shifts, leading to unstable inference.",
            "expected_effect": "Aligns the problem statement directly with the main issue (stability) and justifies the need for a dynamic solution."
          },
          "method_skeleton": {
            "issue": "The language is passive and process-oriented ('Re-analyze,' 'Utilize') rather than algorithmic. It fails to describe *how* the gate learns stability or the specific training objective.",
            "edit_instruction": "Rewrite to describe the algorithmic flow: 'We formulate a reliability loss function that minimizes audio-text discrepancy variance. The gating mechanism is trained via reinforcement learning to maximize reward on reliable joint features.'",
            "expected_effect": "Transforms a vague description into a concrete technical contribution, addressing the low Methodology score."
          },
          "innovation_claims": {
            "issue": "The term 'Text Interference' sounds metaphorical rather than a novel technical contribution. The claims overlap significantly between 'routing' and 'handling misleading text.'",
            "edit_instruction": "Differentiate the claims: 1) A novel 'Modulation Instability Index' (quantifying interference), 2) A 'Reliability-Gated Fusion' architecture, and 3) Demonstration of robustness on zero-shot adversarial splits.",
            "expected_effect": "Clarifies the distinct contributions (metric + architecture + evaluation) and sharpens the novelty."
          },
          "experiments_plan": {
            "issue": "The definition of 'arbitrary modality conditions' is operationally vague. It lacks a specific metric to measure 'stability' beyond standard Accuracy/F1.",
            "edit_instruction": "Define the construction of adversarial splits (e.g., 'mismatched transcripts' or 'low SNR audio'). Add a 'Stability Score' metric (e.g., standard deviation of performance across noise levels) to quantify robustness.",
            "expected_effect": "Makes the experimental setup reproducible and directly measures the paper's main claim (stability)."
          }
        },
        "suggested_edits": [
          {
            "field": "innovation_claims",
            "action": "rewrite",
            "content": "1. Introduce a 'Modulation Instability Index' (MII) that quantifies text interference using energy-temporal divergence, providing a mathematical basis for modality trustworthiness.\n2. Propose a 'Reliability-Gated Fusion' architecture that dynamically switches between joint and unimodal experts to minimize inference variance under noisy conditions.\n3. Establish a new benchmark for 'Stable Emotion Recognition' by evaluating performance degradation across systematically constructed adversarial audio-text mismatches."
          },
          {
            "field": "method_skeleton",
            "action": "rewrite",
            "content": "Formulate a reliability-aware inference pipeline where a Gating Network predicts a confidence score based on the alignment between audio spectral features and text embeddings. Construct a 'Text Interference' loss function that penalizes high cross-modal discrepancy during training. Implement a Dynamic Routing strategy that directs the input to a Joint Multimodal Expert when confidence > threshold, or an Audio-only Expert otherwise, ensuring stable feature integration."
          }
        ],
        "priority": [
          "method_skeleton",
          "innovation_claims",
          "abstract"
        ]
      }
    }
  ],
  "results_dir": "results/run_20260205_034712_9029_8e2252",
  "novelty_report": {
    "run_id": "run_20260205_034712_9029_8e2252",
    "created_at": "2026-02-05T04:15:28.524439+00:00",
    "user_idea": "我们研究音频-文本大模型的文本主导偏见：在 faithful/adversarial/irrelevant 三类设定下，提出基于模态可信度的路由/融合框架。通过构造“文本干扰”表示能量与时序证据，训练轻量门控/适配器，在推理时选择 Joint 或 Audio 专家，实现对抗文本误导下的稳健情绪识别",
    "embedding_available": true,
    "embedding_model": "Qwen/Qwen3-Embedding-8B",
    "top_k": 100,
    "thresholds": {
      "high": 0.88,
      "medium": 0.82
    },
    "risk_level": "low",
    "max_similarity": 0.5874177813529968,
    "candidates": [
      {
        "paper_id": "ePJrZLIqpV",
        "title": "Rethinking Audio-Visual Adversarial Vulnerability from Temporal and Modality Perspectives",
        "pattern_id": "",
        "domain": "Machine Learning",
        "text_hash": "d3f943cf44bf7e970363fa0aedc8716afdd67cc46e915d54ff2861ebe664249d",
        "cosine": 0.5874177813529968,
        "keyword_overlap": 0.112
      },
      {
        "paper_id": "TPZRq4FALB",
        "title": "Test-time Adaptation against Multi-modal Reliability Bias",
        "pattern_id": "",
        "domain": "Machine Learning",
        "text_hash": "85ecbb7da83a1aaae886311195041898f0bd5a40aeffc5097b95fc79d213ed50",
        "cosine": 0.5869333744049072,
        "keyword_overlap": 0.10612244897959183
      },
      {
        "paper_id": "AV7OXVlAyi",
        "title": "Mitigating Modality Prior-Induced Hallucinations in Multimodal Large Language Models via Deciphering Attention Causality",
        "pattern_id": "pattern_13",
        "domain": "Machine Learning",
        "text_hash": "2b00d0adacffa9b4b76e94a68ebe21328497754c613a608494520d4cccc2d32e",
        "cosine": 0.5845937728881836,
        "keyword_overlap": 0.13168724279835392
      },
      {
        "paper_id": "1SYUKPeM12",
        "title": "Aligned Better, Listen Better for Audio-Visual Large Language Models",
        "pattern_id": "pattern_13",
        "domain": "Machine Learning",
        "text_hash": "e80f050a72eb521e9a39c1ad0e8bf7d83712f9be0580d5b59057d0d81b9fcc27",
        "cosine": 0.575981855392456,
        "keyword_overlap": 0.108
      },
      {
        "paper_id": "jTEKTdI3K9",
        "title": "AVHBench: A Cross-Modal Hallucination Benchmark for Audio-Visual Large Language Models",
        "pattern_id": "pattern_13",
        "domain": "Machine Learning",
        "text_hash": "f46910c6a6945456aa4c0dba4aa96ee893bbd85f0ed1a42faf6d3c9a926a10f7",
        "cosine": 0.5593721866607666,
        "keyword_overlap": 0.11336032388663968
      },
      {
        "paper_id": "l60EM8md3t",
        "title": "Revisiting Deep Audio-Text Retrieval Through the Lens of Transportation",
        "pattern_id": "",
        "domain": "Machine Learning",
        "text_hash": "283fd505312f835e0c070c65fdd13e3d83bcd45f433e9e03aed7bb9a3e4e5108",
        "cosine": 0.5464891195297241,
        "keyword_overlap": 0.08536585365853659
      },
      {
        "paper_id": "74vnDs1R97",
        "title": "Wayward Concepts In Multimodal Models",
        "pattern_id": "pattern_51",
        "domain": "Machine Learning",
        "text_hash": "e3575cced82c01c31692bb5ad0e1b47a531a5a79bf5f4da6c81d24b34c00224a",
        "cosine": 0.545717716217041,
        "keyword_overlap": 0.06854838709677419
      },
      {
        "paper_id": "vtT09dYPGI",
        "title": "Routing Experts: Learning to Route Dynamic Experts in Existing Multi-modal Large Language Models",
        "pattern_id": "pattern_74",
        "domain": "Machine Learning",
        "text_hash": "59b22728a0b85817d085cc72b342dae6e768ff39578b9a159327d2d8445e6d8c",
        "cosine": 0.5416673421859741,
        "keyword_overlap": 0.124
      },
      {
        "paper_id": "t851DsVVtA",
        "title": "A Mathematical Framework for Characterizing Dependency Structures of Multimodal Learning",
        "pattern_id": "pattern_113",
        "domain": "Machine Learning",
        "text_hash": "4e027a61c403125f8de2a108a6fc52205d7bcf0020c1d8e8e42571afe3d97019",
        "cosine": 0.5276158452033997,
        "keyword_overlap": 0.09561752988047809
      },
      {
        "paper_id": "rTDyN8yajn",
        "title": "Octavius: Mitigating Task Interference in MLLMs via LoRA-MoE",
        "pattern_id": "",
        "domain": "Machine Learning",
        "text_hash": "80b14c084ad1e87644887c71b1c490518cf88c908295e6af1ef55a1f196f00b1",
        "cosine": 0.527193546295166,
        "keyword_overlap": 0.09787234042553192
      }
    ],
    "notes": [
      "index_reused"
    ],
    "report_path": "results/run_20260205_034712_9029_8e2252/novelty_report.json",
    "pivot_attempts": 0,
    "action": "pivot"
  },
  "recall_audit": {
    "final_top_k": [
      {
        "pattern_id": "pattern_113",
        "name": "Reframing Multimodal Learning Narratives",
        "final_score": 0.7507725793944418,
        "path1_score": 0.7507725793944418,
        "path2_score": 0.0,
        "path3_score": 0.0,
        "cluster_size": 46
      },
      {
        "pattern_id": "pattern_112",
        "name": "Reframing Multimodal Reasoning Challenges",
        "final_score": 0.5847246418220828,
        "path1_score": 0.5847246418220828,
        "path2_score": 0.0,
        "path3_score": 0.0,
        "cluster_size": 115
      },
      {
        "pattern_id": "pattern_13",
        "name": "Hallucination Mitigation via Multimodal Alignment",
        "final_score": 0.20435501362405192,
        "path1_score": 0.20435501362405192,
        "path2_score": 0.0,
        "path3_score": 0.0,
        "cluster_size": 30
      },
      {
        "pattern_id": "pattern_67",
        "name": "Adversarial Robustness Reframing and Dynamics",
        "final_score": 0.2001812535646745,
        "path1_score": 0.16249921730898076,
        "path2_score": 0.0,
        "path3_score": 0.037682036255693746,
        "cluster_size": 68
      },
      {
        "pattern_id": "pattern_74",
        "name": "Democratizing Large Language Model Accessibility",
        "final_score": 0.19485496844549904,
        "path1_score": 0.19485496844549904,
        "path2_score": 0.0,
        "path3_score": 0.0,
        "cluster_size": 156
      },
      {
        "pattern_id": "pattern_8",
        "name": "Reframing Speech Synthesis Efficiency",
        "final_score": 0.1685633712132631,
        "path1_score": 0.16710931986208744,
        "path2_score": 0.00145405135117565,
        "path3_score": 0.0,
        "cluster_size": 33
      },
      {
        "pattern_id": "pattern_44",
        "name": "Privacy Risks Beyond Memorization",
        "final_score": 0.15311372007548119,
        "path1_score": 0.15311372007548119,
        "path2_score": 0.0,
        "path3_score": 0.0,
        "cluster_size": 20
      },
      {
        "pattern_id": "pattern_33",
        "name": "Transformer Training Stability Paradigms",
        "final_score": 0.14752701788782813,
        "path1_score": 0.14752701788782813,
        "path2_score": 0.0,
        "path3_score": 0.0,
        "cluster_size": 31
      },
      {
        "pattern_id": "pattern_9",
        "name": "Provable Fairness Guarantees in Learning",
        "final_score": 0.008624884402748717,
        "path1_score": 0.0,
        "path2_score": 0.008624884402748717,
        "path3_score": 0.0,
        "cluster_size": 74
      },
      {
        "pattern_id": "pattern_7",
        "name": "Reframing Audio Understanding Through Multimodal and Probabilistic Learning",
        "final_score": 0.004847630892639414,
        "path1_score": 0.0,
        "path2_score": 0.004847630892639414,
        "path3_score": 0.0,
        "cluster_size": 41
      }
    ],
    "path1": {
      "top_ideas": [
        {
          "idea_id": "idea_5003",
          "similarity": 0.5123558390235424,
          "snippet": "Introduce a comprehensive framework to evaluate and enhance the adversarial robustness of multimodal language model agents in real environments.",
          "pattern_count": 0
        },
        {
          "idea_id": "idea_7635",
          "similarity": 0.5108875340601298,
          "snippet": "Introduce a causal inference framework to address modality prior-induced biases in multimodal large language models by focusing on attention mechanism causality.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_8256",
          "similarity": 0.5106149795175761,
          "snippet": "Introduce a spatial-aware model to generate controllable and immersive stereo audio from text, leveraging a large-scale multimodal dataset.",
          "pattern_count": 0
        },
        {
          "idea_id": "idea_4407",
          "similarity": 0.5050887514534775,
          "snippet": "Introduce a framework that leverages the synergy between multimodal comprehension and creation to enhance the capabilities of Multimodal Large Language Models.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_4372",
          "similarity": 0.4999906319283259,
          "snippet": "Introduce a unified tokenizer that enables large language models to process text and images interchangeably, enhancing multimodal comprehension and generation.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_1280",
          "similarity": 0.4911014189623583,
          "snippet": "Identify and quantify modality complementariness as a key factor affecting the robustness of multi-modal models.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_6838",
          "similarity": 0.4871374211137476,
          "snippet": "Introduce a dynamic expert routing method to optimize path selection in multimodal large language models without altering their structure.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_7805",
          "similarity": 0.4672634100871593,
          "snippet": "Introduce a multimodal learning strategy that captures redundant, unique, and synergistic information by maximizing mutual information between augmented multimodal features.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_1372",
          "similarity": 0.46121377291682397,
          "snippet": "Introduce a mathematical framework to analytically characterize conditional dependency structures in multimodal learning, considering sample size and task complexity.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_7100",
          "similarity": 0.45735284651976255,
          "snippet": "Introduce a dynamic context sparsification framework to enhance the efficiency of multimodal large language models without degrading performance.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_8000",
          "similarity": 0.45673222117340356,
          "snippet": "Investigate the underlying causes of modality gap and object bias in contrastive vision-language models, revealing information imbalance as a key factor.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_5748",
          "similarity": 0.4177732996552186,
          "snippet": "Introduce a self-supervised learning framework to create syllabic embeddings from raw audio, enhancing speech representation efficiency and robustness.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_587",
          "similarity": 0.40624804327245184,
          "snippet": "Enhance model robustness by integrating a principled abstain mechanism that ensures robustness only when accuracy is maintained, allowing compositional architectures to improve both metrics.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_7996",
          "similarity": 0.4039676498325584,
          "snippet": "Introduce a novel ensembling approach that focuses on the union of top-k tokens to enhance compatibility and efficiency in large language model ensembles.",
          "pattern_count": 0
        },
        {
          "idea_id": "idea_6998",
          "similarity": 0.3995427965645857,
          "snippet": "Introduce a multi-token assisted decoding framework that accelerates and enhances the effectiveness of large language model inference by approximating joint distributions with a smaller auxiliary model.",
          "pattern_count": 0
        },
        {
          "idea_id": "idea_6062",
          "similarity": 0.3870791165399766,
          "snippet": "Introduce a dual process strategy to balance in-context and in-weights learning in language models, enhancing their adaptability and generalization.",
          "pattern_count": 0
        },
        {
          "idea_id": "idea_5241",
          "similarity": 0.38278430018870296,
          "snippet": "Investigate and quantify the extent of non-adversarial memorization in language models, highlighting differences between model and human text reproduction.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_7662",
          "similarity": 0.3763959598055235,
          "snippet": "Introduce a causal framework and data augmentation technique to train reward models that distinguish between contextual signals and irrelevant artifacts.",
          "pattern_count": 0
        },
        {
          "idea_id": "idea_195",
          "similarity": 0.37240848499587914,
          "snippet": "Introduce a mechanism that modulates perception in neural networks based on high-level categorical expectations to enhance accuracy and contextual consistency.",
          "pattern_count": 0
        },
        {
          "idea_id": "idea_7051",
          "similarity": 0.3688175447195703,
          "snippet": "Introduce a dynamic gating mechanism for Transformers that auto-tunes the number of active experts, optimizing efficiency and performance.",
          "pattern_count": 1
        }
      ],
      "pattern_scores_topn": [
        {
          "pattern_id": "pattern_113",
          "score": 0.7507725793944418
        },
        {
          "pattern_id": "pattern_112",
          "score": 0.5847246418220828
        },
        {
          "pattern_id": "pattern_13",
          "score": 0.20435501362405192
        },
        {
          "pattern_id": "pattern_74",
          "score": 0.19485496844549904
        },
        {
          "pattern_id": "pattern_8",
          "score": 0.16710931986208744
        },
        {
          "pattern_id": "pattern_67",
          "score": 0.16249921730898076
        },
        {
          "pattern_id": "pattern_44",
          "score": 0.15311372007548119
        },
        {
          "pattern_id": "pattern_33",
          "score": 0.14752701788782813
        }
      ]
    },
    "path2": {
      "top_domains": [
        {
          "domain_id": "domain_45",
          "name": "Audio Processing",
          "weight": 0.40762303915028547,
          "paper_count": 5
        },
        {
          "domain_id": "domain_82",
          "name": "Speech Recognition",
          "weight": 0.381580951043874,
          "paper_count": 2
        },
        {
          "domain_id": "domain_48",
          "name": "Speech Processing",
          "weight": 0.3696375188833147,
          "paper_count": 7
        },
        {
          "domain_id": "domain_0",
          "name": "Fairness & Accountability",
          "weight": 0.3273453441658455,
          "paper_count": 69
        },
        {
          "domain_id": "domain_57",
          "name": "Signal Processing",
          "weight": 0.3106446556131894,
          "paper_count": 1
        }
      ],
      "top_subdomains": [
        {
          "domain_id": "domain_45",
          "subdomains": [
            {
              "name": "Diffusion Models",
              "score": 0.2792288515040844
            },
            {
              "name": "Contrastive Learning",
              "score": 0.26752351478797537
            }
          ]
        },
        {
          "domain_id": "domain_82",
          "subdomains": [
            {
              "name": "Speech Synthesis",
              "score": 0.2967803415633518
            },
            {
              "name": "Contrastive Learning",
              "score": 0.26649370295457425
            }
          ]
        },
        {
          "domain_id": "domain_48",
          "subdomains": [
            {
              "name": "Speech Synthesis",
              "score": 0.2975207144079297
            },
            {
              "name": "Diffusion Models",
              "score": 0.27932973189284405
            },
            {
              "name": "Contrastive Learning",
              "score": 0.2667562165155647
            }
          ]
        },
        {
          "domain_id": "domain_0",
          "subdomains": [
            {
              "name": "Out-of-Distribution Detection",
              "score": 0.3282614638501798
            },
            {
              "name": "Bias Mitigation",
              "score": 0.3173983617709599
            },
            {
              "name": "Robustness",
              "score": 0.2774564325007078
            },
            {
              "name": "Contrastive Learning",
              "score": 0.26649370295457425
            },
            {
              "name": "Federated Learning",
              "score": 0.24273643169271106
            }
          ]
        },
        {
          "domain_id": "domain_57",
          "subdomains": [
            {
              "name": "Diffusion Models",
              "score": 0.2793297323174929
            },
            {
              "name": "Contrastive Learning",
              "score": 0.26649370295457425
            }
          ]
        }
      ],
      "candidate_stats": [
        {
          "domain_id": "domain_45",
          "candidates_before": 1,
          "candidates_after": 1
        },
        {
          "domain_id": "domain_82",
          "candidates_before": 1,
          "candidates_after": 1
        },
        {
          "domain_id": "domain_48",
          "candidates_before": 2,
          "candidates_after": 2
        },
        {
          "domain_id": "domain_0",
          "candidates_before": 5,
          "candidates_after": 5
        },
        {
          "domain_id": "domain_57",
          "candidates_before": 1,
          "candidates_after": 1
        }
      ],
      "pattern_scores_topn": [
        {
          "pattern_id": "pattern_9",
          "score": 0.008624884402748717
        },
        {
          "pattern_id": "pattern_7",
          "score": 0.004847630892639414
        },
        {
          "pattern_id": "pattern_8",
          "score": 0.00145405135117565
        },
        {
          "pattern_id": "pattern_87",
          "score": 0.0013044006180788006
        },
        {
          "pattern_id": "pattern_45",
          "score": 0.0012545082466614522
        }
      ],
      "subdomain_taxonomy_used": true,
      "raw_subdomain_count": 319,
      "canonical_subdomain_count": 58,
      "stoplist_count": 12
    },
    "path3": {
      "top_papers": [
        {
          "paper_id": "pfuqQQCB34",
          "similarity": 0.25924583262763595,
          "title": "Variance Reduction is an Antidote to Byzantines: Better Rates, Weaker Assumptions and Communication Compression as a Cherry on the Top",
          "quality": 0.6858333333333334,
          "review_count": 6
        },
        {
          "paper_id": "dlQIh4mUtQ8",
          "similarity": 0.28394184823264607,
          "title": "On the Relationship Between Adversarial Robustness and Decision Region in Deep Neural Networks",
          "quality": 0.576,
          "review_count": 5
        }
      ],
      "pattern_scores_topn": [
        {
          "pattern_id": "pattern_67",
          "score": 0.037682036255693746
        }
      ]
    }
  },
  "review_summary": {
    "total_reviews": 1,
    "final_score": 6.803333333333232
  },
  "refinement_summary": {
    "total_refinements": 0,
    "issues_addressed": []
  },
  "verification_summary": {
    "collision_detected": false,
    "max_similarity": 0.5874177813529968
  },
  "idea_packaging": {
    "raw_idea": "我们研究音频-文本大模型的文本主导偏见：在 faithful/adversarial/irrelevant 三类设定下，提出基于模态可信度的路由/融合框架。通过构造“文本干扰”表示能量与时序证据，训练轻量门控/适配器，在推理时选择 Joint 或 Audio 专家，实现对抗文本误导下的稳健情绪识别",
    "brief_a": {
      "motivation": "Existing Audio-Text Large Models suffer from 'text-dominant bias,' where the model over-relies on textual input, leading to performance degradation in emotion recognition when text is adversarial or irrelevant. Addressing this bias is critical for achieving robust multimodal understanding in real-world scenarios where modalities may conflict.",
      "problem_definition": "Task: Robust Multimodal Emotion Recognition. Input: Audio and Text pairs. Output: Emotion labels. Settings: Three distinct scenarios are considered: Faithful (consistent modalities), Adversarial (text misleads), and Irrelevant (text unrelated to audio).",
      "constraints": [
        "robustness (against text bias)",
        "efficiency (lightweight modules/gates)",
        "modality imbalance"
      ],
      "technical_plan": "1. Construct a 'Text Interference' representation utilizing energy and temporal evidence to quantify modality reliability. 2. Train a lightweight gating mechanism or adapter to assess this reliability. 3. Implement a dynamic routing strategy during inference to select between a Joint Multimodal Expert (for faithful cases) and an Audio-only Expert (for adversarial/irrelevant cases) to ensure robust predictions.",
      "expected_contributions": [
        "Identification and mitigation of text-dominant bias in audio-text LLMs",
        "A novel modality reliability-based routing framework utilizing energy and temporal evidence",
        "Improved robustness in emotion recognition under adversarial and irrelevant text settings"
      ],
      "evaluation_plan": "Datasets: Standard multimodal emotion datasets (e.g., IEMOCAP, MELD) with constructed adversarial/irrelevant splits. Metrics: Accuracy, Weighted F1-score. Baselines: Standard Audio-Text LLMs, static fusion methods. Ablations: Impact of the gate, routing strategy, and interference representation. Robustness Settings: Performance comparison specifically on Adversarial and Irrelevant subsets.",
      "keywords_en": [
        "Audio-Text Large Models",
        "Text-Dominant Bias",
        "Multimodal Emotion Recognition",
        "Modality Routing",
        "Robustness"
      ],
      "keywords_zh": [
        "音频-文本大模型",
        "文本主导偏见",
        "多模态情绪识别",
        "模态路由",
        "鲁棒性"
      ],
      "assumptions": {
        "explicit": [
          "The model operates in faithful, adversarial, and irrelevant settings",
          "The solution involves a routing/fusion framework based on modality reliability",
          "Text interference is represented by energy and temporal evidence",
          "Lightweight gates/adapters are used for training",
          "Inference involves selecting between Joint or Audio experts"
        ],
        "inferred": [
          "Audio features contain sufficient information for emotion recognition when text is unreliable (inferred)",
          "'Energy' refers to acoustic energy features (inferred)"
        ]
      }
    },
    "query_a": "Keywords: Audio-Text Large Models, Text-Dominant Bias, Multimodal Emotion Recognition, Modality Routing, Robustness Task: Robust Multimodal Emotion Recognition. Input: Audio and Text pairs. Output: Emotion labels. Settings: Three distinct scenarios are considered: Faithful (consistent modalities), Adversarial (text misleads), and Irrelevant (text unrelated to audio). Constraints: robustness (against text bias), efficiency (lightweight modules/gates), modality imbalance 1. Construct a 'Text Interference' representation utilizing energy and temporal evidence to quantify modality reliability. 2. Train a lightweight gating mechanism or adapter to assess this reliability. 3. Implement a dynamic routing strategy during inference to select between a Joint Multimodal Expert (for faithful cases) and an Audio-only Expert (for adversarial/irrelevant cases) to ensure robust predictions. 关键词: 音频-文本大模型，文本主导偏见，多模态情绪识别，模态路由，鲁棒性",
    "candidates": [
      {
        "pattern_id": "pattern_113",
        "pattern_name": "Reframing Multimodal Learning Narratives",
        "score": 0.7703478034714625,
        "brief": {
          "motivation": "Existing Audio-Text Large Models (AT-LLMs) suffer from a 'text-dominant bias,' where the model over-relies on textual input, leading to performance degradation in emotion recognition when text is adversarial or irrelevant. This issue highlights a fundamental challenge in multimodal learning: managing cross-modality discrepancies and dependency structures where one modality overwhelms the other. Addressing this bias is critical for achieving robust multimodal understanding, reframing the problem from simple fusion to dynamic handling of arbitrary modality conditions (e.g., misleading text) by leveraging reliable uni-modal evidence.",
          "problem_definition": "Task: Robust Multimodal Emotion Recognition. Input: Audio and Text pairs. Output: Emotion labels. Settings: Three distinct scenarios are considered: Faithful (consistent modalities), Adversarial (text misleads), and Irrelevant (text unrelated to audio). The core problem is to mitigate the text-dominant bias that arises from imbalanced conditional dependency structures, ensuring the model remains robust when modalities conflict or when text provides misleading evidence.",
          "constraints": [
            "robustness (against text bias)",
            "efficiency (lightweight modules/gates)",
            "modality imbalance"
          ],
          "technical_plan": "1. Construct a 'Text Interference' representation utilizing energy and temporal evidence to quantify modality reliability and characterize cross-modality discrepancies. 2. Train a lightweight gating mechanism or adapter to assess this reliability, effectively learning the dependency structure between audio and text. 3. Implement a dynamic routing strategy during inference to select between a Joint Multimodal Expert (for faithful cases) and an Audio-only Expert (for adversarial/irrelevant cases), thereby reframing the fusion process as a conditional decision based on modality trustworthiness.",
          "expected_contributions": [
            "Identification and mitigation of text-dominant bias in audio-text LLMs by characterizing modality dependency structures",
            "A novel modality reliability-based routing framework utilizing energy and temporal evidence to handle arbitrary modality conditions",
            "Improved robustness in emotion recognition under adversarial and irrelevant text settings, demonstrating the value of strong uni-modal feature learning when cross-modal alignment fails"
          ],
          "evaluation_plan": "Datasets: Standard multimodal emotion datasets (e.g., IEMOCAP, MELD) with constructed adversarial/irrelevant splits to simulate arbitrary modality conditions. Metrics: Accuracy, Weighted F1-score. Baselines: Standard Audio-Text LLMs, static fusion methods, and uni-modal experts. Ablations: Impact of the gate, routing strategy, and interference representation. Robustness Settings: Performance comparison specifically on Adversarial and Irrelevant subsets to validate the reduction of cross-modality discrepancies.",
          "keywords_en": [
            "Audio-Text Large Models",
            "Text-Dominant Bias",
            "Multimodal Emotion Recognition",
            "Modality Routing",
            "Robustness",
            "Cross-Modality Discrepancy",
            "Arbitrary Modality Conditions"
          ],
          "keywords_zh": [
            "音频-文本大模型",
            "文本主导偏见",
            "多模态情绪识别",
            "模态路由",
            "鲁棒性",
            "跨模态差异",
            "任意模态条件"
          ],
          "assumptions": {
            "explicit": [
              "The model operates in faithful, adversarial, and irrelevant settings",
              "The solution involves a routing/fusion framework based on modality reliability",
              "Text interference is represented by energy and temporal evidence",
              "Lightweight gates/adapters are used for training",
              "Inference involves selecting between Joint or Audio experts"
            ],
            "inferred": [
              "Audio features contain sufficient information for emotion recognition when text is unreliable",
              "'Energy' refers to acoustic energy features",
              "The 'Text Interference' representation can effectively distinguish between faithful and adversarial contexts"
            ]
          }
        },
        "query": "Keywords: Audio-Text Large Models, Text-Dominant Bias, Multimodal Emotion Recognition, Modality Routing, Robustness, Cross-Modality Discrepancy, Arbitrary Modality Conditions Task: Robust Multimodal Emotion Recognition. Input: Audio and Text pairs. Output: Emotion labels. Settings: Three distinct scenarios are considered: Faithful (consistent modalities), Adversarial (text misleads), and Irrelevant (text unrelated to audio). The core problem is to mitigate the text-dominant bias that arises from imbalanced conditional dependency structures, ensuring the model remains robust when modalities conflict or when text provides misleading evidence. Constraints: robustness (against text bias), efficiency (lightweight modules/gates), modality imbalance 1. Construct a 'Text Interference' representation utilizing energy and temporal evidence to quantify modality reliability and characterize cross-modality discrepancies. 2. Train a lightweight gating mechanism or adapter to assess this reliability, effectively learning the dependency structure between audio and text. 3. Implement a dynamic routing strategy during inference to select between a Joint Multimodal Expert (for faithful cases) and an Audio-only Expert (for adversarial/irrelevant cases), thereby reframing the fusion process as a conditional decision based on modality trustworthiness. 关键词: 音频-文本大模型，文本主导偏见，多模态情绪识别，模态路由，鲁棒性，跨模态差异，任意模态条件"
      },
      {
        "pattern_id": "pattern_112",
        "pattern_name": "Reframing Multimodal Reasoning Challenges",
        "score": 0.7498867432339372,
        "brief": {
          "motivation": "Existing Audio-Text Large Models suffer from 'text-dominant bias,' where the model over-relies on textual input, leading to performance degradation in emotion recognition when text is adversarial or irrelevant. Addressing this requires reframing multimodal reasoning from static fusion to a dynamic, modular composition approach. By treating modality selection as a routing problem based on reliability evidence, we can enhance robustness and generalization in real-world scenarios where modalities conflict, aligning with the need for efficient and adaptable multimodal systems that avoid resource-intensive retraining.",
          "problem_definition": "Task: Robust Multimodal Emotion Recognition. Input: Audio and Text pairs. Output: Emotion labels. Settings: Three distinct scenarios are considered: Faithful (consistent modalities), Adversarial (text misleads), and Irrelevant (text unrelated to audio). The core challenge is maintaining high generalization capacity and reasoning integrity when multimodal inputs are inconsistent, a common failure mode in current generic multimodal reasoning systems.",
          "constraints": [
            "robustness (against text bias)",
            "efficiency (lightweight modules/gates to avoid heavy retraining)",
            "modality imbalance",
            "modularity (dynamic routing between experts)"
          ],
          "technical_plan": "1. Construct a 'Text Interference' representation utilizing energy and temporal evidence as an intermediate signal to quantify modality reliability. 2. Train a lightweight gating mechanism or adapter to assess this reliability without full model retraining. 3. Implement a dynamic routing strategy (modular composition) during inference to select between a Joint Multimodal Expert (for faithful cases) and an Audio-only Expert (for adversarial/irrelevant cases), ensuring robust predictions by reframing the fusion process as a conditional selection task.",
          "expected_contributions": [
            "Identification and mitigation of text-dominant bias in audio-text LLMs",
            "A novel modality reliability-based routing framework that reframes multimodal fusion as a dynamic expert selection problem",
            "Improved robustness and generalization capacity in emotion recognition under adversarial and irrelevant text settings"
          ],
          "evaluation_plan": "Datasets: Standard multimodal emotion datasets (e.g., IEMOCAP, MELD) with constructed adversarial/irrelevant splits to test generalization. Metrics: Accuracy, Weighted F1-score. Baselines: Standard Audio-Text LLMs, static fusion methods. Ablations: Impact of the gate, routing strategy, and interference representation. Robustness Settings: Performance comparison specifically on Adversarial and Irrelevant subsets to evaluate generalization capacity under modality conflict.",
          "keywords_en": [
            "Audio-Text Large Models",
            "Text-Dominant Bias",
            "Multimodal Emotion Recognition",
            "Modality Routing",
            "Robustness",
            "Modular Composition",
            "Dynamic Fusion"
          ],
          "keywords_zh": [
            "音频-文本大模型",
            "文本主导偏见",
            "多模态情绪识别",
            "模态路由",
            "鲁棒性",
            "模块化组合",
            "动态融合"
          ],
          "assumptions": {
            "explicit": [
              "The model operates in faithful, adversarial, and irrelevant settings",
              "The solution involves a routing/fusion framework based on modality reliability",
              "Text interference is represented by energy and temporal evidence",
              "Lightweight gates/adapters are used for training",
              "Inference involves selecting between Joint or Audio experts"
            ],
            "inferred": [
              "Audio features contain sufficient information for emotion recognition when text is unreliable",
              "'Energy' refers to acoustic energy features",
              "The 'interference representation' effectively captures the semantic conflict between modalities",
              "The routing mechanism can be trained efficiently without degrading the base model's performance on faithful data"
            ]
          }
        },
        "query": "Keywords: Audio-Text Large Models, Text-Dominant Bias, Multimodal Emotion Recognition, Modality Routing, Robustness, Modular Composition, Dynamic Fusion Task: Robust Multimodal Emotion Recognition. Input: Audio and Text pairs. Output: Emotion labels. Settings: Three distinct scenarios are considered: Faithful (consistent modalities), Adversarial (text misleads), and Irrelevant (text unrelated to audio). The core challenge is maintaining high generalization capacity and reasoning integrity when multimodal inputs are inconsistent, a common failure mode in current generic multimodal reasoning systems. Constraints: robustness (against text bias), efficiency (lightweight modules/gates to avoid heavy retraining), modality imbalance, modularity (dynamic routing between experts) 1. Construct a 'Text Interference' representation utilizing energy and temporal evidence as an intermediate signal to quantify modality reliability. 2. Train a lightweight gating mechanism or adapter to assess this reliability without full model retraining. 3. Implement a dynamic routing strategy (modular composition) during inference to select between a Joint Multimodal Expert (for faithful cases) and an Audio-only Expert (for adversarial/irrelevant cases), ensuring robust predictions by reframing the fusion process as a conditional selection task. 关键词: 音频-文本大模型，文本主导偏见，多模态情绪识别，模态路由，鲁棒性，模块化组合，动态融合"
      },
      {
        "pattern_id": "pattern_74",
        "pattern_name": "Democratizing Large Language Model Accessibility",
        "score": 0.5294783426615813,
        "brief": {
          "motivation": "Existing Audio-Text Large Models (AT-LLMs) suffer from 'text-dominant bias,' where the model over-relies on textual input, leading to performance degradation in emotion recognition when text is adversarial or irrelevant. Addressing this bias is critical not only for robustness but also for the efficient deployment of multimodal LLMs. By leveraging parameter-efficient techniques, we aim to enhance model reliability without the prohibitive computational costs associated with full fine-tuning, aligning with the broader goal of democratizing efficient and robust multimodal AI.",
          "problem_definition": "Task: Robust Multimodal Emotion Recognition. Input: Audio and Text pairs. Output: Emotion labels. Settings: Three distinct scenarios are considered: Faithful (consistent modalities), Adversarial (text misleads), and Irrelevant (text unrelated to audio). The core problem is the model's inability to dynamically weigh modalities, leading to inefficiency and errors when modalities conflict.",
          "constraints": [
            "robustness (against text bias)",
            "efficiency (lightweight modules/gates/parameter-efficient tuning)",
            "modality imbalance",
            "computational accessibility (avoiding full model retraining)"
          ],
          "technical_plan": "1. Construct a 'Text Interference' representation utilizing energy and temporal evidence to quantify modality reliability. 2. Train a lightweight gating mechanism or adapter (inspired by parameter-efficient tuning methods) to assess this reliability without modifying the entire backbone. 3. Implement a dynamic routing strategy during inference to select between a Joint Multimodal Expert (for faithful cases) and an Audio-only Expert (for adversarial/irrelevant cases), ensuring robust predictions while optimizing computational resource usage.",
          "expected_contributions": [
            "Identification and mitigation of text-dominant bias in audio-text LLMs via efficient routing",
            "A novel modality reliability-based routing framework utilizing energy and temporal evidence",
            "Improved robustness in emotion recognition under adversarial and irrelevant text settings",
            "Demonstration of how lightweight adapters can enhance robustness, contributing to efficient multimodal model deployment"
          ],
          "evaluation_plan": "Datasets: Standard multimodal emotion datasets (e.g., IEMOCAP, MELD) with constructed adversarial/irrelevant splits. Metrics: Accuracy, Weighted F1-score, and Efficiency Metrics (e.g., FLOPs, Inference Latency, Number of Trainable Parameters). Baselines: Standard Audio-Text LLMs, static fusion methods, and other parameter-efficient tuning (PET) methods. Ablations: Impact of the gate, routing strategy, and interference representation. Robustness Settings: Performance comparison specifically on Adversarial and Irrelevant subsets.",
          "keywords_en": [
            "Audio-Text Large Models",
            "Text-Dominant Bias",
            "Multimodal Emotion Recognition",
            "Modality Routing",
            "Parameter-Efficient Tuning",
            "Robustness",
            "Model Efficiency"
          ],
          "keywords_zh": [
            "音频-文本大模型",
            "文本主导偏见",
            "多模态情绪识别",
            "模态路由",
            "参数高效微调",
            "鲁棒性",
            "模型效率"
          ],
          "assumptions": {
            "explicit": [
              "The model operates in faithful, adversarial, and irrelevant settings",
              "The solution involves a routing/fusion framework based on modality reliability",
              "Text interference is represented by energy and temporal evidence",
              "Lightweight gates/adapters are used for training to ensure efficiency",
              "Inference involves selecting between Joint or Audio experts"
            ],
            "inferred": [
              "Audio features contain sufficient information for emotion recognition when text is unreliable",
              "'Energy' refers to acoustic energy features",
              "The lightweight gating mechanism functions as a form of Parameter-Efficient Tuning (PET)",
              "Dynamic routing reduces computational cost compared to always running full multimodal fusion"
            ]
          }
        },
        "query": "Keywords: Audio-Text Large Models, Text-Dominant Bias, Multimodal Emotion Recognition, Modality Routing, Parameter-Efficient Tuning, Robustness, Model Efficiency Task: Robust Multimodal Emotion Recognition. Input: Audio and Text pairs. Output: Emotion labels. Settings: Three distinct scenarios are considered: Faithful (consistent modalities), Adversarial (text misleads), and Irrelevant (text unrelated to audio). The core problem is the model's inability to dynamically weigh modalities, leading to inefficiency and errors when modalities conflict. Constraints: robustness (against text bias), efficiency (lightweight modules/gates/parameter-efficient tuning), modality imbalance, computational accessibility (avoiding full model retraining) 1. Construct a 'Text Interference' representation utilizing energy and temporal evidence to quantify modality reliability. 2. Train a lightweight gating mechanism or adapter (inspired by parameter-efficient tuning methods) to assess this reliability without modifying the entire backbone. 3. Implement a dynamic routing strategy during inference to select between a Joint Multimodal Expert (for faithful cases) and an Audio-only Expert (for adversarial/irrelevant cases), ensuring robust predictions while optimizing computational resource usage. 关键词: 音频-文本大模型，文本主导偏见，多模态情绪识别，模态路由，参数高效微调，鲁棒性，模型效率"
      }
    ],
    "judge": {
      "best_index": 0,
      "rationale": "Candidate 0 (pattern_113) is the best choice because it most faithfully captures the core intent of the raw idea: addressing 'text-dominant bias' in Audio-Text LLMs through a reliability-based routing framework. \n\n1. **Faithfulness**: It accurately identifies the problem as a 'text-dominant bias' arising from 'imbalanced conditional dependency structures,' which aligns perfectly with the user's description of the issue. Unlike Candidate 2 (pattern_74), it does not force a 'democratization/accessibility' narrative that is secondary to the main goal of robustness. \n\n2. **Completeness**: The candidate provides a comprehensive structure, explicitly detailing the three settings (faithful, adversarial, irrelevant) and the specific methodology (constructing 'Text Interference' via energy/temporal evidence, training lightweight gates, and dynamic routing). \n\n3. **Clarity and Actionability**: The technical plan is precise, mentioning the characterization of 'cross-modality discrepancies' and the specific routing logic (Joint vs. Audio expert). The evaluation plan is robust, including specific datasets (IEMOCAP, MELD), metrics, and ablation studies relevant to the bias problem. \n\nWhile Candidate 1 (pattern_112) is also strong, Candidate 0 provides a slightly more precise technical characterization of the bias problem (dependency structures vs. general reasoning), making it the most accurate reflection of the raw idea."
    },
    "recall_scores": {
      "0": 0.0,
      "1": 0.0,
      "2": 0.0
    },
    "chosen_index": 0,
    "query_best": "Keywords: Audio-Text Large Models, Text-Dominant Bias, Multimodal Emotion Recognition, Modality Routing, Robustness, Cross-Modality Discrepancy, Arbitrary Modality Conditions Task: Robust Multimodal Emotion Recognition. Input: Audio and Text pairs. Output: Emotion labels. Settings: Three distinct scenarios are considered: Faithful (consistent modalities), Adversarial (text misleads), and Irrelevant (text unrelated to audio). The core problem is to mitigate the text-dominant bias that arises from imbalanced conditional dependency structures, ensuring the model remains robust when modalities conflict or when text provides misleading evidence. Constraints: robustness (against text bias), efficiency (lightweight modules/gates), modality imbalance 1. Construct a 'Text Interference' representation utilizing energy and temporal evidence to quantify modality reliability and characterize cross-modality discrepancies. 2. Train a lightweight gating mechanism or adapter to assess this reliability, effectively learning the dependency structure between audio and text. 3. Implement a dynamic routing strategy during inference to select between a Joint Multimodal Expert (for faithful cases) and an Audio-only Expert (for adversarial/irrelevant cases), thereby reframing the fusion process as a conditional decision based on modality trustworthiness. 关键词: 音频-文本大模型，文本主导偏见，多模态情绪识别，模态路由，鲁棒性，跨模态差异，任意模态条件",
    "brief_best": {
      "motivation": "Existing Audio-Text Large Models (AT-LLMs) suffer from a 'text-dominant bias,' where the model over-relies on textual input, leading to performance degradation in emotion recognition when text is adversarial or irrelevant. This issue highlights a fundamental challenge in multimodal learning: managing cross-modality discrepancies and dependency structures where one modality overwhelms the other. Addressing this bias is critical for achieving robust multimodal understanding, reframing the problem from simple fusion to dynamic handling of arbitrary modality conditions (e.g., misleading text) by leveraging reliable uni-modal evidence.",
      "problem_definition": "Task: Robust Multimodal Emotion Recognition. Input: Audio and Text pairs. Output: Emotion labels. Settings: Three distinct scenarios are considered: Faithful (consistent modalities), Adversarial (text misleads), and Irrelevant (text unrelated to audio). The core problem is to mitigate the text-dominant bias that arises from imbalanced conditional dependency structures, ensuring the model remains robust when modalities conflict or when text provides misleading evidence.",
      "constraints": [
        "robustness (against text bias)",
        "efficiency (lightweight modules/gates)",
        "modality imbalance"
      ],
      "technical_plan": "1. Construct a 'Text Interference' representation utilizing energy and temporal evidence to quantify modality reliability and characterize cross-modality discrepancies. 2. Train a lightweight gating mechanism or adapter to assess this reliability, effectively learning the dependency structure between audio and text. 3. Implement a dynamic routing strategy during inference to select between a Joint Multimodal Expert (for faithful cases) and an Audio-only Expert (for adversarial/irrelevant cases), thereby reframing the fusion process as a conditional decision based on modality trustworthiness.",
      "expected_contributions": [
        "Identification and mitigation of text-dominant bias in audio-text LLMs by characterizing modality dependency structures",
        "A novel modality reliability-based routing framework utilizing energy and temporal evidence to handle arbitrary modality conditions",
        "Improved robustness in emotion recognition under adversarial and irrelevant text settings, demonstrating the value of strong uni-modal feature learning when cross-modal alignment fails"
      ],
      "evaluation_plan": "Datasets: Standard multimodal emotion datasets (e.g., IEMOCAP, MELD) with constructed adversarial/irrelevant splits to simulate arbitrary modality conditions. Metrics: Accuracy, Weighted F1-score. Baselines: Standard Audio-Text LLMs, static fusion methods, and uni-modal experts. Ablations: Impact of the gate, routing strategy, and interference representation. Robustness Settings: Performance comparison specifically on Adversarial and Irrelevant subsets to validate the reduction of cross-modality discrepancies.",
      "keywords_en": [
        "Audio-Text Large Models",
        "Text-Dominant Bias",
        "Multimodal Emotion Recognition",
        "Modality Routing",
        "Robustness",
        "Cross-Modality Discrepancy",
        "Arbitrary Modality Conditions"
      ],
      "keywords_zh": [
        "音频-文本大模型",
        "文本主导偏见",
        "多模态情绪识别",
        "模态路由",
        "鲁棒性",
        "跨模态差异",
        "任意模态条件"
      ],
      "assumptions": {
        "explicit": [
          "The model operates in faithful, adversarial, and irrelevant settings",
          "The solution involves a routing/fusion framework based on modality reliability",
          "Text interference is represented by energy and temporal evidence",
          "Lightweight gates/adapters are used for training",
          "Inference involves selecting between Joint or Audio experts"
        ],
        "inferred": [
          "Audio features contain sufficient information for emotion recognition when text is unreliable",
          "'Energy' refers to acoustic energy features",
          "The 'Text Interference' representation can effectively distinguish between faithful and adversarial contexts"
        ]
      }
    }
  }
}